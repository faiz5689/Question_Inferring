{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file created with 143 rows\n",
      "\n",
      "Columns in the combined file:\n",
      "- Id\n",
      "- Title\n",
      "- Body\n",
      "- Title_Body\n",
      "- ImageURLs\n",
      "- llm_zero_shot_title\n",
      "- llm_zero_shot_body\n",
      "- llm_zero_shot_combined\n",
      "- llm_few_shot_title\n",
      "- llm_few_shot_body\n",
      "- llm_few_shot_combined\n",
      "- llm_cot_title\n",
      "- llm_cot_body\n",
      "- llm_cot_combined\n",
      "\n",
      "Sample of first row:\n",
      "\n",
      "Id:\n",
      "79146548\n",
      "\n",
      "Title:\n",
      "GitHub Copilot responds to 'Hey Code' but dictation doesn't work\n",
      "\n",
      "Body:\n",
      "As the title explains, I can start an inline chat session using the 'hey code' voice command in VS C...\n",
      "\n",
      "Title_Body:\n",
      "<<GitHub Copilot responds to 'Hey Code' but dictation doesn't work>>\n",
      "<<As the title explains, I can ...\n",
      "\n",
      "ImageURLs:\n",
      "['https://i.sstatic.net/MgGjdapB.png']\n",
      "\n",
      "llm_zero_shot_title:\n",
      "** Unexpected Null Pointer Exception in Event Handler Setup\n",
      "\n",
      "llm_zero_shot_body:\n",
      "I'm experiencing an issue with my code where the `SetupEventHandlers` method is not being called as ...\n",
      "\n",
      "llm_zero_shot_combined:\n",
      "<<** Unexpected Null Pointer Exception in Event Handler Setup>>\n",
      "<<I'm experiencing an issue with my ...\n",
      "\n",
      "llm_few_shot_title:\n",
      "** \"Error when accessing nested object property in JavaScript\"\n",
      "\n",
      "This title accurately reflects the m...\n",
      "\n",
      "llm_few_shot_body:\n",
      "**\n",
      "\n",
      "I'm having trouble with my code and I'm hoping someone can help me out. I'm trying to use the `s...\n",
      "\n",
      "llm_few_shot_combined:\n",
      "<<** \"Error when accessing nested object property in JavaScript\"\n",
      "\n",
      "This title accurately reflects the...\n",
      "\n",
      "llm_cot_title:\n",
      "Error in SetupEventHandlers() function: \"GPT4o\" is not defined\n",
      "\n",
      "llm_cot_body:\n",
      "I'm experiencing an issue with my code in the screenshot provided. The error message \"SetupEventHand...\n",
      "\n",
      "llm_cot_combined:\n",
      "<<Error in SetupEventHandlers() function: \"GPT4o\" is not defined>>\n",
      "<<I'm experiencing an issue with ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "zero_shot_df = pd.read_csv('Data/llama-3.2/llm_responses_final_zero_shot.csv')\n",
    "few_shot_df = pd.read_csv('Data/llama-3.2/llm_responses_final_few_shot.csv')\n",
    "chain_of_thoughts_df = pd.read_csv('Data/llama-3.2/llm_responses_final_chain_of_thoughts.csv')\n",
    "\n",
    "# Create the combined text column for original title and body\n",
    "zero_shot_df['Title_Body'] = '<<' + zero_shot_df['Title'] + '>>\\n<<' + zero_shot_df['Body'] + '>>'\n",
    "\n",
    "# Create combined response columns for each LLM\n",
    "zero_shot_df['llm_response_combined'] = '<<' + zero_shot_df['llm_title_response'] + '>>\\n<<' + zero_shot_df['llm_body_response'] + '>>'\n",
    "few_shot_df['llm_response_combined'] = '<<' + few_shot_df['llm_title_response'] + '>>\\n<<' + few_shot_df['llm_body_response'] + '>>'\n",
    "chain_of_thoughts_df['llm_response_combined'] = '<<' + chain_of_thoughts_df['llm_title_response'] + '>>\\n<<' + chain_of_thoughts_df['llm_body_response'] + '>>'\n",
    "\n",
    "# Create the final dataframe with all columns\n",
    "combined_df = pd.DataFrame({\n",
    "    'Id': zero_shot_df['Id'],\n",
    "    'Title': zero_shot_df['Title'],\n",
    "    'Body': zero_shot_df['Body'],\n",
    "    'Title_Body': zero_shot_df['Title_Body'],\n",
    "    'ImageURLs': zero_shot_df['ImageURLs'],\n",
    "    # Zero-shot responses\n",
    "    'llm_zero_shot_title': zero_shot_df['llm_title_response'],\n",
    "    'llm_zero_shot_body': zero_shot_df['llm_body_response'],\n",
    "    'llm_zero_shot_combined': zero_shot_df['llm_response_combined'],\n",
    "    # Few-shot responses\n",
    "    'llm_few_shot_title': few_shot_df['llm_title_response'],\n",
    "    'llm_few_shot_body': few_shot_df['llm_body_response'],\n",
    "    'llm_few_shot_combined': few_shot_df['llm_response_combined'],\n",
    "    # Chain of thoughts responses\n",
    "    'llm_cot_title': chain_of_thoughts_df['llm_title_response'],\n",
    "    'llm_cot_body': chain_of_thoughts_df['llm_body_response'],\n",
    "    'llm_cot_combined': chain_of_thoughts_df['llm_response_combined']\n",
    "})\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv('Data/llama-3.2/llm_responses_combined.csv', index=False)\n",
    "\n",
    "# Print basic information about the combined file\n",
    "print(f\"Combined file created with {len(combined_df)} rows\")\n",
    "print(\"\\nColumns in the combined file:\")\n",
    "for col in combined_df.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "# Print a sample row to verify the structure\n",
    "print(\"\\nSample of first row:\")\n",
    "for col in combined_df.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(str(combined_df[col].iloc[0])[:100] + \"...\" if len(str(combined_df[col].iloc[0])) > 100 else str(combined_df[col].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Title_Body</th>\n",
       "      <th>ImageURLs</th>\n",
       "      <th>llm_zero_shot_title</th>\n",
       "      <th>llm_zero_shot_body</th>\n",
       "      <th>llm_zero_shot_combined</th>\n",
       "      <th>llm_few_shot_title</th>\n",
       "      <th>llm_few_shot_body</th>\n",
       "      <th>llm_few_shot_combined</th>\n",
       "      <th>llm_cot_title</th>\n",
       "      <th>llm_cot_body</th>\n",
       "      <th>llm_cot_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79146548</td>\n",
       "      <td>GitHub Copilot responds to 'Hey Code' but dict...</td>\n",
       "      <td>As the title explains, I can start an inline c...</td>\n",
       "      <td>&lt;&lt;GitHub Copilot responds to 'Hey Code' but di...</td>\n",
       "      <td>['https://i.sstatic.net/MgGjdapB.png']</td>\n",
       "      <td>** Unexpected Null Pointer Exception in Event ...</td>\n",
       "      <td>I'm experiencing an issue with my code where t...</td>\n",
       "      <td>&lt;&lt;** Unexpected Null Pointer Exception in Even...</td>\n",
       "      <td>** \"Error when accessing nested object propert...</td>\n",
       "      <td>**\\n\\nI'm having trouble with my code and I'm ...</td>\n",
       "      <td>&lt;&lt;** \"Error when accessing nested object prope...</td>\n",
       "      <td>Error in SetupEventHandlers() function: \"GPT4o...</td>\n",
       "      <td>I'm experiencing an issue with my code in the ...</td>\n",
       "      <td>&lt;&lt;Error in SetupEventHandlers() function: \"GPT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79146419</td>\n",
       "      <td>How can I fix my Workflow file to successfully...</td>\n",
       "      <td>I am trying to use Github Actions with Azure S...</td>\n",
       "      <td>&lt;&lt;How can I fix my Workflow file to successful...</td>\n",
       "      <td>['https://i.sstatic.net/THwNK2Jj.png']</td>\n",
       "      <td>Unhandled Exception in ASP.NET Core Web API - ...</td>\n",
       "      <td>I'm experiencing an issue with my project wher...</td>\n",
       "      <td>&lt;&lt;Unhandled Exception in ASP.NET Core Web API ...</td>\n",
       "      <td>**\\n\"Error in SQL Query: 'Invalid column name'...</td>\n",
       "      <td>**\\n\\nI am experiencing an issue with my code ...</td>\n",
       "      <td>&lt;&lt;**\\n\"Error in SQL Query: 'Invalid column nam...</td>\n",
       "      <td>Error in RStudio: \"Error in file(con, \"r\") : c...</td>\n",
       "      <td>I'm trying to implement a custom authenticatio...</td>\n",
       "      <td>&lt;&lt;Error in RStudio: \"Error in file(con, \"r\") :...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79146412</td>\n",
       "      <td>LINQPad 8 Dump Property Order different that L...</td>\n",
       "      <td>In LINQPad 5 with Linq-to-Sql DataContext, if ...</td>\n",
       "      <td>&lt;&lt;LINQPad 8 Dump Property Order different that...</td>\n",
       "      <td>['https://i.sstatic.net/efq4SfvI.png']</td>\n",
       "      <td>Unhandled Exception in ASP.NET MVC Application...</td>\n",
       "      <td>I'm experiencing an issue with my project wher...</td>\n",
       "      <td>&lt;&lt;Unhandled Exception in ASP.NET MVC Applicati...</td>\n",
       "      <td>\"How to fix 'No module named 'gpt2' error in P...</td>\n",
       "      <td>**\\n\\nI'm experiencing an issue with my Python...</td>\n",
       "      <td>&lt;&lt;\"How to fix 'No module named 'gpt2' error in...</td>\n",
       "      <td>Error in SkiaSharp NuGet Package: 'SkiaSharp' ...</td>\n",
       "      <td>I'm trying to implement a feature in my applic...</td>\n",
       "      <td>&lt;&lt;Error in SkiaSharp NuGet Package: 'SkiaSharp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79146127</td>\n",
       "      <td>SyntaxError: Cannot use import statement outsi...</td>\n",
       "      <td>I'm using TypeScript, ESM, npm, and ts-jest. U...</td>\n",
       "      <td>&lt;&lt;SyntaxError: Cannot use import statement out...</td>\n",
       "      <td>['https://i.sstatic.net/Jp5wj6k2.png']</td>\n",
       "      <td>Error in Wena-Test-Runner: Command Failed with...</td>\n",
       "      <td>I'm experiencing an issue with my project wher...</td>\n",
       "      <td>&lt;&lt;Error in Wena-Test-Runner: Command Failed wi...</td>\n",
       "      <td>** \"Syntax Error: Cannot use import statement ...</td>\n",
       "      <td>**\\n\\nI'm having trouble with a Python script ...</td>\n",
       "      <td>&lt;&lt;** \"Syntax Error: Cannot use import statemen...</td>\n",
       "      <td>JavaScript Syntax Error on Line 2: Unexpected ...</td>\n",
       "      <td>I'm having trouble with my WENA test runner. W...</td>\n",
       "      <td>&lt;&lt;JavaScript Syntax Error on Line 2: Unexpecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79145758</td>\n",
       "      <td>Typescript Polymorphic Component Event Handler</td>\n",
       "      <td>I have written a strongly-typed Polymorphic Ty...</td>\n",
       "      <td>&lt;&lt;Typescript Polymorphic Component Event Handl...</td>\n",
       "      <td>['https://i.sstatic.net/19LCKEF3.png']</td>\n",
       "      <td>** Unhandled Exception: \"Property 'currentTarg...</td>\n",
       "      <td>I'm experiencing an issue with my code where t...</td>\n",
       "      <td>&lt;&lt;** Unhandled Exception: \"Property 'currentTa...</td>\n",
       "      <td>\"How to Create a Dynamic Dropdown List in Exce...</td>\n",
       "      <td>**\\n\\nI'm experiencing an issue with my Python...</td>\n",
       "      <td>&lt;&lt;\"How to Create a Dynamic Dropdown List in Ex...</td>\n",
       "      <td>Error in HTML Element Reference: \"Property 'cu...</td>\n",
       "      <td>I'm experiencing an issue with my code in the ...</td>\n",
       "      <td>&lt;&lt;Error in HTML Element Reference: \"Property '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  79146548  GitHub Copilot responds to 'Hey Code' but dict...   \n",
       "1  79146419  How can I fix my Workflow file to successfully...   \n",
       "2  79146412  LINQPad 8 Dump Property Order different that L...   \n",
       "3  79146127  SyntaxError: Cannot use import statement outsi...   \n",
       "4  79145758     Typescript Polymorphic Component Event Handler   \n",
       "\n",
       "                                                Body  \\\n",
       "0  As the title explains, I can start an inline c...   \n",
       "1  I am trying to use Github Actions with Azure S...   \n",
       "2  In LINQPad 5 with Linq-to-Sql DataContext, if ...   \n",
       "3  I'm using TypeScript, ESM, npm, and ts-jest. U...   \n",
       "4  I have written a strongly-typed Polymorphic Ty...   \n",
       "\n",
       "                                          Title_Body  \\\n",
       "0  <<GitHub Copilot responds to 'Hey Code' but di...   \n",
       "1  <<How can I fix my Workflow file to successful...   \n",
       "2  <<LINQPad 8 Dump Property Order different that...   \n",
       "3  <<SyntaxError: Cannot use import statement out...   \n",
       "4  <<Typescript Polymorphic Component Event Handl...   \n",
       "\n",
       "                                ImageURLs  \\\n",
       "0  ['https://i.sstatic.net/MgGjdapB.png']   \n",
       "1  ['https://i.sstatic.net/THwNK2Jj.png']   \n",
       "2  ['https://i.sstatic.net/efq4SfvI.png']   \n",
       "3  ['https://i.sstatic.net/Jp5wj6k2.png']   \n",
       "4  ['https://i.sstatic.net/19LCKEF3.png']   \n",
       "\n",
       "                                 llm_zero_shot_title  \\\n",
       "0  ** Unexpected Null Pointer Exception in Event ...   \n",
       "1  Unhandled Exception in ASP.NET Core Web API - ...   \n",
       "2  Unhandled Exception in ASP.NET MVC Application...   \n",
       "3  Error in Wena-Test-Runner: Command Failed with...   \n",
       "4  ** Unhandled Exception: \"Property 'currentTarg...   \n",
       "\n",
       "                                  llm_zero_shot_body  \\\n",
       "0  I'm experiencing an issue with my code where t...   \n",
       "1  I'm experiencing an issue with my project wher...   \n",
       "2  I'm experiencing an issue with my project wher...   \n",
       "3  I'm experiencing an issue with my project wher...   \n",
       "4  I'm experiencing an issue with my code where t...   \n",
       "\n",
       "                              llm_zero_shot_combined  \\\n",
       "0  <<** Unexpected Null Pointer Exception in Even...   \n",
       "1  <<Unhandled Exception in ASP.NET Core Web API ...   \n",
       "2  <<Unhandled Exception in ASP.NET MVC Applicati...   \n",
       "3  <<Error in Wena-Test-Runner: Command Failed wi...   \n",
       "4  <<** Unhandled Exception: \"Property 'currentTa...   \n",
       "\n",
       "                                  llm_few_shot_title  \\\n",
       "0  ** \"Error when accessing nested object propert...   \n",
       "1  **\\n\"Error in SQL Query: 'Invalid column name'...   \n",
       "2  \"How to fix 'No module named 'gpt2' error in P...   \n",
       "3  ** \"Syntax Error: Cannot use import statement ...   \n",
       "4  \"How to Create a Dynamic Dropdown List in Exce...   \n",
       "\n",
       "                                   llm_few_shot_body  \\\n",
       "0  **\\n\\nI'm having trouble with my code and I'm ...   \n",
       "1  **\\n\\nI am experiencing an issue with my code ...   \n",
       "2  **\\n\\nI'm experiencing an issue with my Python...   \n",
       "3  **\\n\\nI'm having trouble with a Python script ...   \n",
       "4  **\\n\\nI'm experiencing an issue with my Python...   \n",
       "\n",
       "                               llm_few_shot_combined  \\\n",
       "0  <<** \"Error when accessing nested object prope...   \n",
       "1  <<**\\n\"Error in SQL Query: 'Invalid column nam...   \n",
       "2  <<\"How to fix 'No module named 'gpt2' error in...   \n",
       "3  <<** \"Syntax Error: Cannot use import statemen...   \n",
       "4  <<\"How to Create a Dynamic Dropdown List in Ex...   \n",
       "\n",
       "                                       llm_cot_title  \\\n",
       "0  Error in SetupEventHandlers() function: \"GPT4o...   \n",
       "1  Error in RStudio: \"Error in file(con, \"r\") : c...   \n",
       "2  Error in SkiaSharp NuGet Package: 'SkiaSharp' ...   \n",
       "3  JavaScript Syntax Error on Line 2: Unexpected ...   \n",
       "4  Error in HTML Element Reference: \"Property 'cu...   \n",
       "\n",
       "                                        llm_cot_body  \\\n",
       "0  I'm experiencing an issue with my code in the ...   \n",
       "1  I'm trying to implement a custom authenticatio...   \n",
       "2  I'm trying to implement a feature in my applic...   \n",
       "3  I'm having trouble with my WENA test runner. W...   \n",
       "4  I'm experiencing an issue with my code in the ...   \n",
       "\n",
       "                                    llm_cot_combined  \n",
       "0  <<Error in SetupEventHandlers() function: \"GPT...  \n",
       "1  <<Error in RStudio: \"Error in file(con, \"r\") :...  \n",
       "2  <<Error in SkiaSharp NuGet Package: 'SkiaSharp...  \n",
       "3  <<JavaScript Syntax Error on Line 2: Unexpecte...  \n",
       "4  <<Error in HTML Element Reference: \"Property '...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for original content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f1e49d598b4e6194431576632dc4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8737d5d6caff44eea887e5dd2c771479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec6280dc42948cbad3c7366455a340b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for LLM responses...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada238f616c841c58204592e5de7e700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6863e6844a94f84a2ceac379aa1b6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0611917779b34918a865270a465a9e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e652613cefa44216bc117b084931bbce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73f20ad665e42518eea93bf17a1ddcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e20e192377e487980c142b2e9361a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cafa71d38164d0490e96d73db4e95fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514503ce3874448f9b1e7e53d68c59c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401b55528a934a4c9ae635657d53a1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TITLE Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.2018\n",
      "median: 0.1416\n",
      "std: 0.2105\n",
      "min: -0.1047\n",
      "max: 0.9072\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 1.40%\n",
      "High: 3.50%\n",
      "Moderate: 11.89%\n",
      "Low: 23.78%\n",
      "Very Low: 46.15%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.0433\n",
      "median: 0.0312\n",
      "std: 0.1076\n",
      "min: -0.1458\n",
      "max: 0.5320\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.00%\n",
      "High: 0.00%\n",
      "Moderate: 2.80%\n",
      "Low: 2.80%\n",
      "Very Low: 61.54%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.2100\n",
      "median: 0.1468\n",
      "std: 0.2110\n",
      "min: -0.0901\n",
      "max: 0.9859\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 1.40%\n",
      "High: 5.59%\n",
      "Moderate: 12.59%\n",
      "Low: 20.98%\n",
      "Very Low: 48.95%\n",
      "\n",
      "BODY Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.2535\n",
      "median: 0.2073\n",
      "std: 0.2123\n",
      "min: -0.1515\n",
      "max: 0.8100\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.70%\n",
      "High: 6.99%\n",
      "Moderate: 17.48%\n",
      "Low: 27.27%\n",
      "Very Low: 39.86%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.0500\n",
      "median: 0.0331\n",
      "std: 0.0978\n",
      "min: -0.1428\n",
      "max: 0.4029\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.00%\n",
      "High: 0.00%\n",
      "Moderate: 0.70%\n",
      "Low: 6.99%\n",
      "Very Low: 60.84%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.2676\n",
      "median: 0.2342\n",
      "std: 0.2166\n",
      "min: -0.1339\n",
      "max: 0.7913\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.00%\n",
      "High: 9.09%\n",
      "Moderate: 19.58%\n",
      "Low: 26.57%\n",
      "Very Low: 35.66%\n",
      "\n",
      "COMBINED Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.2768\n",
      "median: 0.2515\n",
      "std: 0.2140\n",
      "min: -0.0473\n",
      "max: 0.7681\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.00%\n",
      "High: 11.89%\n",
      "Moderate: 16.78%\n",
      "Low: 30.07%\n",
      "Very Low: 33.57%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.0506\n",
      "median: 0.0382\n",
      "std: 0.0967\n",
      "min: -0.1472\n",
      "max: 0.3069\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.00%\n",
      "High: 0.00%\n",
      "Moderate: 0.00%\n",
      "Low: 7.69%\n",
      "Very Low: 62.24%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.3099\n",
      "median: 0.2994\n",
      "std: 0.2152\n",
      "min: -0.1026\n",
      "max: 0.8176\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.70%\n",
      "High: 13.99%\n",
      "Moderate: 17.48%\n",
      "Low: 33.57%\n",
      "Very Low: 27.97%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_embeddings_and_analyze(df, model_name='sentence-transformers/all-mpnet-base-v1'):\n",
    "    \"\"\"\n",
    "    Create embeddings using SentenceTransformer and analyze similarities for both\n",
    "    separate title/body responses and combined responses\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings for original content\n",
    "    print(\"Generating embeddings for original content...\")\n",
    "    title_embeddings = model.encode(df['Title'].tolist(), show_progress_bar=True)\n",
    "    body_embeddings = model.encode(df['Body'].tolist(), show_progress_bar=True)\n",
    "    title_body_embeddings = model.encode(df['Title_Body'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    # Generate embeddings for LLM responses\n",
    "    print(\"\\nGenerating embeddings for LLM responses...\")\n",
    "    response_embeddings = {\n",
    "        # Title responses\n",
    "        'zero_shot_title': model.encode(df['llm_zero_shot_title'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_title': model.encode(df['llm_few_shot_title'].tolist(), show_progress_bar=True),\n",
    "        'cot_title': model.encode(df['llm_cot_title'].tolist(), show_progress_bar=True),\n",
    "        \n",
    "        # Body responses\n",
    "        'zero_shot_body': model.encode(df['llm_zero_shot_body'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_body': model.encode(df['llm_few_shot_body'].tolist(), show_progress_bar=True),\n",
    "        'cot_body': model.encode(df['llm_cot_body'].tolist(), show_progress_bar=True),\n",
    "        \n",
    "        # Combined responses\n",
    "        'zero_shot_combined': model.encode(df['llm_zero_shot_combined'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_combined': model.encode(df['llm_few_shot_combined'].tolist(), show_progress_bar=True),\n",
    "        'cot_combined': model.encode(df['llm_cot_combined'].tolist(), show_progress_bar=True)\n",
    "    }\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate title similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['title'][response_type] = np.diagonal(\n",
    "            cosine_similarity(title_embeddings, response_embeddings[f'{response_type}_title'])\n",
    "        )\n",
    "    \n",
    "    # Calculate body similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['body'][response_type] = np.diagonal(\n",
    "            cosine_similarity(body_embeddings, response_embeddings[f'{response_type}_body'])\n",
    "        )\n",
    "    \n",
    "    # Calculate combined similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['combined'][response_type] = np.diagonal(\n",
    "            cosine_similarity(title_body_embeddings, response_embeddings[f'{response_type}_combined'])\n",
    "        )\n",
    "    \n",
    "    # Add similarity scores to dataframe\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        df[f'similarity_{response_type}_title'] = similarities['title'][response_type]\n",
    "        df[f'similarity_{response_type}_body'] = similarities['body'][response_type]\n",
    "        df[f'similarity_{response_type}_combined'] = similarities['combined'][response_type]\n",
    "    \n",
    "    # Save embeddings with updated paths\n",
    "    np.save('Data/llama-3.2/title_embeddings_st.npy', title_embeddings)\n",
    "    np.save('Data/llama-3.2/body_embeddings_st.npy', body_embeddings)\n",
    "    np.save('Data/llama-3.2/title_body_embeddings_st.npy', title_body_embeddings)\n",
    "    \n",
    "    for response_type, embeddings in response_embeddings.items():\n",
    "        np.save(f'Data/llama-3.2/{response_type}_embeddings_st.npy', embeddings)\n",
    "    \n",
    "    return df, similarities\n",
    "\n",
    "def analyze_similarities(similarities):\n",
    "    \"\"\"\n",
    "    Analyze and visualize similarity distributions for title, body, and combined responses\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        'Very High': (0.8, 1.0),\n",
    "        'High': (0.6, 0.8),\n",
    "        'Moderate': (0.4, 0.6),\n",
    "        'Low': (0.2, 0.4),\n",
    "        'Very Low': (0.0, 0.2)\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Create figures for each type of response\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle(f'Similarity Analysis for {response_category.capitalize()} Responses')\n",
    "        \n",
    "        # Plot distributions and calculate statistics for each LLM type\n",
    "        for idx, response_type in enumerate(['zero_shot', 'few_shot', 'cot']):\n",
    "            scores = similarities[response_category][response_type]\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = {\n",
    "                'mean': np.mean(scores),\n",
    "                'median': np.median(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "            \n",
    "            # Calculate distribution across categories\n",
    "            distribution = {}\n",
    "            for category, (low, high) in categories.items():\n",
    "                count = np.sum((scores >= low) & (scores < high))\n",
    "                percentage = (count / len(scores)) * 100\n",
    "                distribution[category] = percentage\n",
    "                \n",
    "            results[response_category][response_type] = {\n",
    "                'statistics': stats,\n",
    "                'distribution': distribution\n",
    "            }\n",
    "            \n",
    "            # Plot histogram\n",
    "            ax = axes[idx]\n",
    "            sns.histplot(scores, bins=30, ax=ax)\n",
    "            ax.set_title(f'{response_type.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel('Similarity Score')\n",
    "            ax.set_ylabel('Count')\n",
    "            \n",
    "            # Add category boundaries\n",
    "            for category, (low, high) in categories.items():\n",
    "                if low > 0:  # Don't plot the lowest boundary\n",
    "                    ax.axvline(x=low, color='r', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        # Updated path for saving plots\n",
    "        plt.savefig(f'Data/llama-3.2/similarity_distributions_{response_category}_st.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Print analysis\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        print(f\"\\n{response_category.upper()} Response Analysis:\")\n",
    "        for response_type, result in results[response_category].items():\n",
    "            print(f\"\\n{response_type.upper()}:\")\n",
    "            print(\"\\nBasic Statistics:\")\n",
    "            for metric, value in result['statistics'].items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            \n",
    "            print(\"\\nDistribution across categories:\")\n",
    "            for category, percentage in result['distribution'].items():\n",
    "                print(f\"{category}: {percentage:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the combined CSV file with updated path\n",
    "    df = pd.read_csv('Data/llama-3.2/llm_responses_combined.csv')\n",
    "    \n",
    "    # Generate embeddings and calculate similarities\n",
    "    df_with_similarities, similarities = create_embeddings_and_analyze(df)\n",
    "    \n",
    "    # Analyze and visualize results\n",
    "    analysis_results = analyze_similarities(similarities)\n",
    "    \n",
    "    # Save updated dataframe with similarities using new path\n",
    "    df_with_similarities.to_csv('Data/llama-3.2/llm_responses_with_similarities_st.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for original content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eac57aaef954b4c9ae1de8e0cf82a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264a94de843a42c8a33462e39abc8e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a556d84552148d1833f51a2d879ead2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for LLM responses...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f819c5aaacc0435690c645e00a76143b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c55a7a3f53464bb89d332f07df904c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08c5aa1cd434fe7bd4451f81cf0c47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc882643f6a8438fbb7552318ec12134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf482c48fe9943d7819a65a61c79c7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac66c472c71401c80f20e7bf5a15d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b012c938df8248f0ad490d35d263415f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcde4ac346344dfbd2a6b33f6f4d62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fa88cc1fd940eca37564607722441b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TITLE Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.2106\n",
      "median: 0.1645\n",
      "std: 0.2167\n",
      "min: -0.1702\n",
      "max: 0.9559\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 1.40%\n",
      "High: 4.90%\n",
      "Moderate: 9.79%\n",
      "Low: 23.08%\n",
      "Very Low: 46.85%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.0436\n",
      "median: 0.0371\n",
      "std: 0.1158\n",
      "min: -0.1474\n",
      "max: 0.6026\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.00%\n",
      "High: 0.70%\n",
      "Moderate: 1.40%\n",
      "Low: 4.90%\n",
      "Very Low: 53.85%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.2195\n",
      "median: 0.1624\n",
      "std: 0.2191\n",
      "min: -0.1978\n",
      "max: 0.9854\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 1.40%\n",
      "High: 5.59%\n",
      "Moderate: 13.29%\n",
      "Low: 22.38%\n",
      "Very Low: 46.15%\n",
      "\n",
      "BODY Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.2753\n",
      "median: 0.2574\n",
      "std: 0.2205\n",
      "min: -0.0921\n",
      "max: 0.8441\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.70%\n",
      "High: 8.39%\n",
      "Moderate: 22.38%\n",
      "Low: 25.17%\n",
      "Very Low: 34.97%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.0554\n",
      "median: 0.0444\n",
      "std: 0.1113\n",
      "min: -0.1731\n",
      "max: 0.4849\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.00%\n",
      "High: 0.00%\n",
      "Moderate: 0.70%\n",
      "Low: 7.69%\n",
      "Very Low: 60.14%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.3013\n",
      "median: 0.2838\n",
      "std: 0.2151\n",
      "min: -0.1250\n",
      "max: 0.9006\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 1.40%\n",
      "High: 8.39%\n",
      "Moderate: 24.48%\n",
      "Low: 26.57%\n",
      "Very Low: 33.57%\n",
      "\n",
      "COMBINED Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.3133\n",
      "median: 0.2946\n",
      "std: 0.2244\n",
      "min: -0.1285\n",
      "max: 0.8757\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 1.40%\n",
      "High: 10.49%\n",
      "Moderate: 23.78%\n",
      "Low: 29.37%\n",
      "Very Low: 27.27%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.0724\n",
      "median: 0.0664\n",
      "std: 0.1047\n",
      "min: -0.1313\n",
      "max: 0.4053\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.00%\n",
      "High: 0.00%\n",
      "Moderate: 0.70%\n",
      "Low: 12.59%\n",
      "Very Low: 60.84%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.3565\n",
      "median: 0.3639\n",
      "std: 0.2069\n",
      "min: -0.1056\n",
      "max: 0.8610\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 1.40%\n",
      "High: 11.89%\n",
      "Moderate: 28.67%\n",
      "Low: 31.47%\n",
      "Very Low: 24.48%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_embeddings_and_analyze(df, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Create embeddings using SentenceTransformer and analyze similarities for both\n",
    "    separate title/body responses and combined responses\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings for original content\n",
    "    print(\"Generating embeddings for original content...\")\n",
    "    title_embeddings = model.encode(df['Title'].tolist(), show_progress_bar=True)\n",
    "    body_embeddings = model.encode(df['Body'].tolist(), show_progress_bar=True)\n",
    "    title_body_embeddings = model.encode(df['Title_Body'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    # Generate embeddings for LLM responses\n",
    "    print(\"\\nGenerating embeddings for LLM responses...\")\n",
    "    response_embeddings = {\n",
    "        # Title responses\n",
    "        'zero_shot_title': model.encode(df['llm_zero_shot_title'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_title': model.encode(df['llm_few_shot_title'].tolist(), show_progress_bar=True),\n",
    "        'cot_title': model.encode(df['llm_cot_title'].tolist(), show_progress_bar=True),\n",
    "        \n",
    "        # Body responses\n",
    "        'zero_shot_body': model.encode(df['llm_zero_shot_body'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_body': model.encode(df['llm_few_shot_body'].tolist(), show_progress_bar=True),\n",
    "        'cot_body': model.encode(df['llm_cot_body'].tolist(), show_progress_bar=True),\n",
    "        \n",
    "        # Combined responses\n",
    "        'zero_shot_combined': model.encode(df['llm_zero_shot_combined'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_combined': model.encode(df['llm_few_shot_combined'].tolist(), show_progress_bar=True),\n",
    "        'cot_combined': model.encode(df['llm_cot_combined'].tolist(), show_progress_bar=True)\n",
    "    }\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate title similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['title'][response_type] = np.diagonal(\n",
    "            cosine_similarity(title_embeddings, response_embeddings[f'{response_type}_title'])\n",
    "        )\n",
    "    \n",
    "    # Calculate body similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['body'][response_type] = np.diagonal(\n",
    "            cosine_similarity(body_embeddings, response_embeddings[f'{response_type}_body'])\n",
    "        )\n",
    "    \n",
    "    # Calculate combined similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['combined'][response_type] = np.diagonal(\n",
    "            cosine_similarity(title_body_embeddings, response_embeddings[f'{response_type}_combined'])\n",
    "        )\n",
    "    \n",
    "    # Add similarity scores to dataframe\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        df[f'similarity_{response_type}_title'] = similarities['title'][response_type]\n",
    "        df[f'similarity_{response_type}_body'] = similarities['body'][response_type]\n",
    "        df[f'similarity_{response_type}_combined'] = similarities['combined'][response_type]\n",
    "    \n",
    "    # Save embeddings with updated paths\n",
    "    np.save('Data/llama-3.2/title_embeddings_st_minilm.npy', title_embeddings)\n",
    "    np.save('Data/llama-3.2/body_embeddings_st_minilm.npy', body_embeddings)\n",
    "    np.save('Data/llama-3.2/title_body_embeddings_st_minilm.npy', title_body_embeddings)\n",
    "    \n",
    "    for response_type, embeddings in response_embeddings.items():\n",
    "        np.save(f'Data/llama-3.2/{response_type}_embeddings_st_minilm.npy', embeddings)\n",
    "    \n",
    "    return df, similarities\n",
    "\n",
    "def analyze_similarities(similarities):\n",
    "    \"\"\"\n",
    "    Analyze and visualize similarity distributions for title, body, and combined responses\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        'Very High': (0.8, 1.0),\n",
    "        'High': (0.6, 0.8),\n",
    "        'Moderate': (0.4, 0.6),\n",
    "        'Low': (0.2, 0.4),\n",
    "        'Very Low': (0.0, 0.2)\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Create figures for each type of response\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle(f'Similarity Analysis for {response_category.capitalize()} Responses')\n",
    "        \n",
    "        # Plot distributions and calculate statistics for each LLM type\n",
    "        for idx, response_type in enumerate(['zero_shot', 'few_shot', 'cot']):\n",
    "            scores = similarities[response_category][response_type]\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = {\n",
    "                'mean': np.mean(scores),\n",
    "                'median': np.median(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "            \n",
    "            # Calculate distribution across categories\n",
    "            distribution = {}\n",
    "            for category, (low, high) in categories.items():\n",
    "                count = np.sum((scores >= low) & (scores < high))\n",
    "                percentage = (count / len(scores)) * 100\n",
    "                distribution[category] = percentage\n",
    "                \n",
    "            results[response_category][response_type] = {\n",
    "                'statistics': stats,\n",
    "                'distribution': distribution\n",
    "            }\n",
    "            \n",
    "            # Plot histogram\n",
    "            ax = axes[idx]\n",
    "            sns.histplot(scores, bins=30, ax=ax)\n",
    "            ax.set_title(f'{response_type.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel('Similarity Score')\n",
    "            ax.set_ylabel('Count')\n",
    "            \n",
    "            # Add category boundaries\n",
    "            for category, (low, high) in categories.items():\n",
    "                if low > 0:  # Don't plot the lowest boundary\n",
    "                    ax.axvline(x=low, color='r', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        # Updated path for saving plots\n",
    "        plt.savefig(f'Data/llama-3.2/similarity_distributions_{response_category}_st_minilm.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Print analysis\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        print(f\"\\n{response_category.upper()} Response Analysis:\")\n",
    "        for response_type, result in results[response_category].items():\n",
    "            print(f\"\\n{response_type.upper()}:\")\n",
    "            print(\"\\nBasic Statistics:\")\n",
    "            for metric, value in result['statistics'].items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            \n",
    "            print(\"\\nDistribution across categories:\")\n",
    "            for category, percentage in result['distribution'].items():\n",
    "                print(f\"{category}: {percentage:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the combined CSV file with updated path\n",
    "    df = pd.read_csv('Data/llama-3.2/llm_responses_combined.csv')\n",
    "    \n",
    "    # Generate embeddings and calculate similarities\n",
    "    df_with_similarities, similarities = create_embeddings_and_analyze(df)\n",
    "    \n",
    "    # Analyze and visualize results\n",
    "    analysis_results = analyze_similarities(similarities)\n",
    "    \n",
    "    # Save updated dataframe with similarities using new path\n",
    "    df_with_similarities.to_csv('Data/llama-3.2/llm_responses_with_similarities_st_minilm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for original content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5328da3298443eb8500831419fb719e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013dbe46d1804d5e9227b8e6d9e9d14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5750738092fe4bb1944a27cf8f267ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for LLM responses...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5217e99f714979b8c56da2aaf22ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a578cebc8372421998647c9c08bac835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b979a39174f4a559b2bb8d1857cbc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059a9df218b245bc9dda547769467f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298624ead8a4480989d0482b28444ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d4913cffd34295b172c4c0d221ad38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19693efbf6844085a280bbab8bf63849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f63747e0f6d43b79ef32e00c37612e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a56d7055bde4ab5be282f2f04635646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TITLE Response Analysis (KartonBERT):\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4826\n",
      "median: 0.4599\n",
      "std: 0.1451\n",
      "min: 0.2217\n",
      "max: 0.9529\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 3.50%\n",
      "High: 16.78%\n",
      "Moderate: 46.85%\n",
      "Low: 32.87%\n",
      "Very Low: 0.00%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.3909\n",
      "median: 0.3729\n",
      "std: 0.0986\n",
      "min: 0.2035\n",
      "max: 0.8134\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.70%\n",
      "High: 3.50%\n",
      "Moderate: 37.06%\n",
      "Low: 58.74%\n",
      "Very Low: 0.00%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4788\n",
      "median: 0.4556\n",
      "std: 0.1485\n",
      "min: 0.2276\n",
      "max: 0.9752\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 2.80%\n",
      "High: 16.08%\n",
      "Moderate: 45.45%\n",
      "Low: 35.66%\n",
      "Very Low: 0.00%\n",
      "\n",
      "BODY Response Analysis (KartonBERT):\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.6003\n",
      "median: 0.5901\n",
      "std: 0.1229\n",
      "min: 0.2488\n",
      "max: 0.9212\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 2.80%\n",
      "High: 45.45%\n",
      "Moderate: 47.55%\n",
      "Low: 4.20%\n",
      "Very Low: 0.00%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4575\n",
      "median: 0.4596\n",
      "std: 0.0822\n",
      "min: 0.2203\n",
      "max: 0.6455\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.00%\n",
      "High: 4.20%\n",
      "Moderate: 72.73%\n",
      "Low: 23.08%\n",
      "Very Low: 0.00%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5935\n",
      "median: 0.5946\n",
      "std: 0.1237\n",
      "min: 0.3109\n",
      "max: 0.9245\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 4.20%\n",
      "High: 44.76%\n",
      "Moderate: 43.36%\n",
      "Low: 7.69%\n",
      "Very Low: 0.00%\n",
      "\n",
      "COMBINED Response Analysis (KartonBERT):\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5860\n",
      "median: 0.5798\n",
      "std: 0.1386\n",
      "min: 0.2337\n",
      "max: 0.9101\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 4.20%\n",
      "High: 43.36%\n",
      "Moderate: 41.96%\n",
      "Low: 10.49%\n",
      "Very Low: 0.00%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4476\n",
      "median: 0.4512\n",
      "std: 0.0814\n",
      "min: 0.2349\n",
      "max: 0.7106\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 0.00%\n",
      "High: 1.40%\n",
      "Moderate: 69.23%\n",
      "Low: 29.37%\n",
      "Very Low: 0.00%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.6019\n",
      "median: 0.6059\n",
      "std: 0.1254\n",
      "min: 0.3335\n",
      "max: 0.9300\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 4.90%\n",
      "High: 46.15%\n",
      "Moderate: 43.36%\n",
      "Low: 5.59%\n",
      "Very Low: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_embeddings_and_analyze(df, model_name='OrlikB/KartonBERT-USE-base-v1'):\n",
    "    \"\"\"\n",
    "    Create embeddings using KartonBERT and analyze similarities with normalized embeddings\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings for original content\n",
    "    print(\"Generating embeddings for original content...\")\n",
    "    title_embeddings = model.encode(df['Title'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    body_embeddings = model.encode(df['Body'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    title_body_embeddings = model.encode(df['Title_Body'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    \n",
    "    # Generate embeddings for LLM responses\n",
    "    print(\"\\nGenerating embeddings for LLM responses...\")\n",
    "    response_embeddings = {\n",
    "        # Title responses\n",
    "        'zero_shot_title': model.encode(df['llm_zero_shot_title'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'few_shot_title': model.encode(df['llm_few_shot_title'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'cot_title': model.encode(df['llm_cot_title'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        \n",
    "        # Body responses\n",
    "        'zero_shot_body': model.encode(df['llm_zero_shot_body'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'few_shot_body': model.encode(df['llm_few_shot_body'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'cot_body': model.encode(df['llm_cot_body'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        \n",
    "        # Combined responses\n",
    "        'zero_shot_combined': model.encode(df['llm_zero_shot_combined'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'few_shot_combined': model.encode(df['llm_few_shot_combined'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'cot_combined': model.encode(df['llm_cot_combined'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    }\n",
    "    \n",
    "    # Calculate similarities using dot product\n",
    "    similarities = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate title similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        # Using dot product for normalized vectors\n",
    "        similarities['title'][response_type] = np.sum(\n",
    "            title_embeddings * response_embeddings[f'{response_type}_title'], axis=1\n",
    "        )\n",
    "    \n",
    "    # Calculate body similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['body'][response_type] = np.sum(\n",
    "            body_embeddings * response_embeddings[f'{response_type}_body'], axis=1\n",
    "        )\n",
    "    \n",
    "    # Calculate combined similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['combined'][response_type] = np.sum(\n",
    "            title_body_embeddings * response_embeddings[f'{response_type}_combined'], axis=1\n",
    "        )\n",
    "    \n",
    "    # Add similarity scores to dataframe\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        df[f'similarity_{response_type}_title'] = similarities['title'][response_type]\n",
    "        df[f'similarity_{response_type}_body'] = similarities['body'][response_type]\n",
    "        df[f'similarity_{response_type}_combined'] = similarities['combined'][response_type]\n",
    "    \n",
    "    # Save embeddings with updated paths\n",
    "    np.save('Data/llama-3.2/title_embeddings_kartonbert.npy', title_embeddings)\n",
    "    np.save('Data/llama-3.2/body_embeddings_kartonbert.npy', body_embeddings)\n",
    "    np.save('Data/llama-3.2/title_body_embeddings_kartonbert.npy', title_body_embeddings)\n",
    "    \n",
    "    for response_type, embeddings in response_embeddings.items():\n",
    "        np.save(f'Data/llama-3.2/{response_type}_embeddings_kartonbert.npy', embeddings)\n",
    "    \n",
    "    return df, similarities\n",
    "\n",
    "def analyze_similarities(similarities):\n",
    "    \"\"\"\n",
    "    Analyze and visualize similarity distributions for title, body, and combined responses\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        'Very High': (0.8, 1.0),\n",
    "        'High': (0.6, 0.8),\n",
    "        'Moderate': (0.4, 0.6),\n",
    "        'Low': (0.2, 0.4),\n",
    "        'Very Low': (0.0, 0.2)\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Create figures for each type of response\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle(f'Similarity Analysis for {response_category.capitalize()} Responses (KartonBERT)')\n",
    "        \n",
    "        # Plot distributions and calculate statistics for each LLM type\n",
    "        for idx, response_type in enumerate(['zero_shot', 'few_shot', 'cot']):\n",
    "            scores = similarities[response_category][response_type]\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = {\n",
    "                'mean': np.mean(scores),\n",
    "                'median': np.median(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "            \n",
    "            # Calculate distribution across categories\n",
    "            distribution = {}\n",
    "            for category, (low, high) in categories.items():\n",
    "                count = np.sum((scores >= low) & (scores < high))\n",
    "                percentage = (count / len(scores)) * 100\n",
    "                distribution[category] = percentage\n",
    "                \n",
    "            results[response_category][response_type] = {\n",
    "                'statistics': stats,\n",
    "                'distribution': distribution\n",
    "            }\n",
    "            \n",
    "            # Plot histogram\n",
    "            ax = axes[idx]\n",
    "            sns.histplot(scores, bins=30, ax=ax)\n",
    "            ax.set_title(f'{response_type.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel('Similarity Score')\n",
    "            ax.set_ylabel('Count')\n",
    "            \n",
    "            # Add category boundaries\n",
    "            for category, (low, high) in categories.items():\n",
    "                if low > 0:  # Don't plot the lowest boundary\n",
    "                    ax.axvline(x=low, color='r', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'Data/llama-3.2/similarity_distributions_{response_category}_kartonbert.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Print analysis\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        print(f\"\\n{response_category.upper()} Response Analysis (KartonBERT):\")\n",
    "        for response_type, result in results[response_category].items():\n",
    "            print(f\"\\n{response_type.upper()}:\")\n",
    "            print(\"\\nBasic Statistics:\")\n",
    "            for metric, value in result['statistics'].items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            \n",
    "            print(\"\\nDistribution across categories:\")\n",
    "            for category, percentage in result['distribution'].items():\n",
    "                print(f\"{category}: {percentage:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the combined CSV file with updated path\n",
    "    df = pd.read_csv('Data/llama-3.2/llm_responses_combined.csv')\n",
    "    \n",
    "    # Generate embeddings and calculate similarities\n",
    "    df_with_similarities, similarities = create_embeddings_and_analyze(df)\n",
    "    \n",
    "    # Analyze and visualize results\n",
    "    analysis_results = analyze_similarities(similarities)\n",
    "    \n",
    "    # Save updated dataframe with similarities using new path\n",
    "    df_with_similarities.to_csv('Data/llama-3.2/llm_responses_with_similarities_kartonbert.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
