{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing zero-shot file with 143 rows\n",
      "Processing few-shot file with 143 rows\n",
      "Processing chain of thoughts file with 143 rows\n",
      "\n",
      "Verification of updated files:\n",
      "\n",
      "Zero-shot file:\n",
      "Columns: ['Id', 'llm_title_response', 'llm_body_response', 'Title', 'Body', 'ImageURLs']\n",
      "Number of rows: 143\n",
      "\n",
      "Sample of first row:\n",
      "Id: 79041624\n",
      "llm_title_response: Issue with Asynchronous Behavior in Node.js Function\n",
      "llm_body_response: I'm working on a Node.js project and encountering an issue with the `convertGrammar` function in my ...\n",
      "Title: How to use switch branch button only seems to work for single repo in multi-root workspace\n",
      "Body: I am used to looking at the source control checkout button in vscode as a quick indication of what b...\n",
      "ImageURLs: ['https://code.visualstudio.com/assets/docs/editor/multi-root-workspaces/hero.png']\n",
      "\n",
      "Few-shot file:\n",
      "Columns: ['Id', 'llm_title_response', 'llm_body_response', 'Title', 'Body', 'ImageURLs']\n",
      "Number of rows: 143\n",
      "\n",
      "Sample of first row:\n",
      "Id: 79041624\n",
      "llm_title_response: How to Properly Use 'require' in Node.js for Module Imports?\n",
      "llm_body_response: I'm working on a Node.js project and using the 'require' function to import modules in my JavaScript...\n",
      "Title: How to use switch branch button only seems to work for single repo in multi-root workspace\n",
      "Body: I am used to looking at the source control checkout button in vscode as a quick indication of what b...\n",
      "ImageURLs: ['https://code.visualstudio.com/assets/docs/editor/multi-root-workspaces/hero.png']\n",
      "\n",
      "Chain of thoughts file:\n",
      "Columns: ['Id', 'llm_title_response', 'llm_body_response', 'Title', 'Body', 'ImageURLs']\n",
      "Number of rows: 143\n",
      "\n",
      "Sample of first row:\n",
      "Id: 79041624\n",
      "llm_title_response: How to Properly Use Promises in Node.js for File Operations?\n",
      "llm_body_response: I'm working on a Node.js project using Visual Studio Code, and I'm trying to handle file operations ...\n",
      "Title: How to use switch branch button only seems to work for single repo in multi-root workspace\n",
      "Body: I am used to looking at the source control checkout button in vscode as a quick indication of what b...\n",
      "ImageURLs: ['https://code.visualstudio.com/assets/docs/editor/multi-root-workspaces/hero.png']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the original combined file for Id mapping\n",
    "original_df = pd.read_csv('Data/llm_responses_combined.csv')\n",
    "\n",
    "# Create a mapping dictionary from Id to other columns\n",
    "id_mapping = original_df.set_index('Id')[['Title', 'Body', 'ImageURLs']].to_dict('index')\n",
    "\n",
    "# Column mapping for renaming\n",
    "column_mapping = {\n",
    "    'ID': 'Id',\n",
    "    'TITLE': 'llm_title_response',\n",
    "    'BODY': 'llm_body_response'\n",
    "}\n",
    "\n",
    "# Read and update each GPT-4 file\n",
    "# Zero-shot file\n",
    "zero_shot_df = pd.read_csv('Data/GPT-4o/GPT4o_incontext_res.csv')\n",
    "print(f\"Processing zero-shot file with {len(zero_shot_df)} rows\")\n",
    "zero_shot_df = zero_shot_df.rename(columns=column_mapping)\n",
    "for column in ['Title', 'Body', 'ImageURLs']:\n",
    "    zero_shot_df[column] = zero_shot_df['Id'].map(lambda x: id_mapping[x][column])\n",
    "zero_shot_df.to_csv('Data/GPT-4o/GPT4o_incontext_res.csv', index=False)\n",
    "\n",
    "# Few-shot file\n",
    "few_shot_df = pd.read_csv('Data/GPT-4o/GPT4o_fewshot_res.csv')\n",
    "print(f\"Processing few-shot file with {len(few_shot_df)} rows\")\n",
    "few_shot_df = few_shot_df.rename(columns=column_mapping)\n",
    "for column in ['Title', 'Body', 'ImageURLs']:\n",
    "    few_shot_df[column] = few_shot_df['Id'].map(lambda x: id_mapping[x][column])\n",
    "few_shot_df.to_csv('Data/GPT-4o/GPT4o_fewshot_res.csv', index=False)\n",
    "\n",
    "# Chain of thoughts file\n",
    "chain_of_thoughts_df = pd.read_csv('Data/GPT-4o/GPT4o_cot_res.csv')\n",
    "print(f\"Processing chain of thoughts file with {len(chain_of_thoughts_df)} rows\")\n",
    "chain_of_thoughts_df = chain_of_thoughts_df.rename(columns=column_mapping)\n",
    "for column in ['Title', 'Body', 'ImageURLs']:\n",
    "    chain_of_thoughts_df[column] = chain_of_thoughts_df['Id'].map(lambda x: id_mapping[x][column])\n",
    "chain_of_thoughts_df.to_csv('Data/GPT-4o/GPT4o_cot_res.csv', index=False)\n",
    "\n",
    "# Print verification information\n",
    "print(\"\\nVerification of updated files:\")\n",
    "for name, df in [(\"Zero-shot\", zero_shot_df), \n",
    "                 (\"Few-shot\", few_shot_df), \n",
    "                 (\"Chain of thoughts\", chain_of_thoughts_df)]:\n",
    "    print(f\"\\n{name} file:\")\n",
    "    print(\"Columns:\", list(df.columns))\n",
    "    print(\"Number of rows:\", len(df))\n",
    "    print(\"\\nSample of first row:\")\n",
    "    for col in df.columns:\n",
    "        val = str(df[col].iloc[0])\n",
    "        print(f\"{col}:\", val[:100] + \"...\" if len(val) > 100 else val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file created with 143 rows\n",
      "\n",
      "Columns in the combined file:\n",
      "- Id\n",
      "- Title\n",
      "- Body\n",
      "- Title_Body\n",
      "- ImageURLs\n",
      "- llm_zero_shot_title\n",
      "- llm_zero_shot_body\n",
      "- llm_zero_shot_combined\n",
      "- llm_few_shot_title\n",
      "- llm_few_shot_body\n",
      "- llm_few_shot_combined\n",
      "- llm_cot_title\n",
      "- llm_cot_body\n",
      "- llm_cot_combined\n",
      "\n",
      "Sample of first row:\n",
      "\n",
      "Id:\n",
      "79041624\n",
      "\n",
      "Title:\n",
      "How to use switch branch button only seems to work for single repo in multi-root workspace\n",
      "\n",
      "Body:\n",
      "I am used to looking at the source control checkout button in vscode as a quick indication of what b...\n",
      "\n",
      "Title_Body:\n",
      "<<How to use switch branch button only seems to work for single repo in multi-root workspace>>\n",
      "<<I a...\n",
      "\n",
      "ImageURLs:\n",
      "['https://code.visualstudio.com/assets/docs/editor/multi-root-workspaces/hero.png']\n",
      "\n",
      "llm_zero_shot_title:\n",
      "Issue with Asynchronous Behavior in Node.js Function\n",
      "\n",
      "llm_zero_shot_body:\n",
      "I'm working on a Node.js project and encountering an issue with the `convertGrammar` function in my ...\n",
      "\n",
      "llm_zero_shot_combined:\n",
      "<<Issue with Asynchronous Behavior in Node.js Function>>\n",
      "<<I'm working on a Node.js project and enco...\n",
      "\n",
      "llm_few_shot_title:\n",
      "How to Properly Use 'require' in Node.js for Module Imports?\n",
      "\n",
      "llm_few_shot_body:\n",
      "I'm working on a Node.js project and using the 'require' function to import modules in my JavaScript...\n",
      "\n",
      "llm_few_shot_combined:\n",
      "<<How to Properly Use 'require' in Node.js for Module Imports?>>\n",
      "<<I'm working on a Node.js project ...\n",
      "\n",
      "llm_cot_title:\n",
      "How to Properly Use Promises in Node.js for File Operations?\n",
      "\n",
      "llm_cot_body:\n",
      "I'm working on a Node.js project using Visual Studio Code, and I'm trying to handle file operations ...\n",
      "\n",
      "llm_cot_combined:\n",
      "<<How to Properly Use Promises in Node.js for File Operations?>>\n",
      "<<I'm working on a Node.js project ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "zero_shot_df = pd.read_csv('Data/GPT-4o/GPT4o_incontext_res.csv')\n",
    "few_shot_df = pd.read_csv('Data/GPT-4o/GPT4o_fewshot_res.csv')\n",
    "chain_of_thoughts_df = pd.read_csv('Data/GPT-4o/GPT4o_cot_res.csv')\n",
    "\n",
    "# Create the combined text column for original title and body\n",
    "zero_shot_df['Title_Body'] = '<<' + zero_shot_df['Title'] + '>>\\n<<' + zero_shot_df['Body'] + '>>'\n",
    "\n",
    "# Create combined response columns for each LLM\n",
    "zero_shot_df['llm_response_combined'] = '<<' + zero_shot_df['llm_title_response'] + '>>\\n<<' + zero_shot_df['llm_body_response'] + '>>'\n",
    "few_shot_df['llm_response_combined'] = '<<' + few_shot_df['llm_title_response'] + '>>\\n<<' + few_shot_df['llm_body_response'] + '>>'\n",
    "chain_of_thoughts_df['llm_response_combined'] = '<<' + chain_of_thoughts_df['llm_title_response'] + '>>\\n<<' + chain_of_thoughts_df['llm_body_response'] + '>>'\n",
    "\n",
    "# Create the final dataframe with all columns\n",
    "combined_df = pd.DataFrame({\n",
    "    'Id': zero_shot_df['Id'],\n",
    "    'Title': zero_shot_df['Title'],\n",
    "    'Body': zero_shot_df['Body'],\n",
    "    'Title_Body': zero_shot_df['Title_Body'],\n",
    "    'ImageURLs': zero_shot_df['ImageURLs'],\n",
    "    # Zero-shot responses\n",
    "    'llm_zero_shot_title': zero_shot_df['llm_title_response'],\n",
    "    'llm_zero_shot_body': zero_shot_df['llm_body_response'],\n",
    "    'llm_zero_shot_combined': zero_shot_df['llm_response_combined'],\n",
    "    # Few-shot responses\n",
    "    'llm_few_shot_title': few_shot_df['llm_title_response'],\n",
    "    'llm_few_shot_body': few_shot_df['llm_body_response'],\n",
    "    'llm_few_shot_combined': few_shot_df['llm_response_combined'],\n",
    "    # Chain of thoughts responses\n",
    "    'llm_cot_title': chain_of_thoughts_df['llm_title_response'],\n",
    "    'llm_cot_body': chain_of_thoughts_df['llm_body_response'],\n",
    "    'llm_cot_combined': chain_of_thoughts_df['llm_response_combined']\n",
    "})\n",
    "\n",
    "# Save the combined dataframe to a new CSV file in the GPT-4o folder\n",
    "combined_df.to_csv('Data/GPT-4o/GPT-4o_responses_combined.csv', index=False)\n",
    "\n",
    "# Print basic information about the combined file\n",
    "print(f\"Combined file created with {len(combined_df)} rows\")\n",
    "print(\"\\nColumns in the combined file:\")\n",
    "for col in combined_df.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "# Print a sample row to verify the structure\n",
    "print(\"\\nSample of first row:\")\n",
    "for col in combined_df.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(str(combined_df[col].iloc[0])[:100] + \"...\" if len(str(combined_df[col].iloc[0])) > 100 else str(combined_df[col].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143 entries, 0 to 142\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   Id                      143 non-null    int64 \n",
      " 1   Title                   143 non-null    object\n",
      " 2   Body                    143 non-null    object\n",
      " 3   Title_Body              143 non-null    object\n",
      " 4   ImageURLs               143 non-null    object\n",
      " 5   llm_zero_shot_title     143 non-null    object\n",
      " 6   llm_zero_shot_body      142 non-null    object\n",
      " 7   llm_zero_shot_combined  142 non-null    object\n",
      " 8   llm_few_shot_title      143 non-null    object\n",
      " 9   llm_few_shot_body       143 non-null    object\n",
      " 10  llm_few_shot_combined   143 non-null    object\n",
      " 11  llm_cot_title           143 non-null    object\n",
      " 12  llm_cot_body            143 non-null    object\n",
      " 13  llm_cot_combined        143 non-null    object\n",
      "dtypes: int64(1), object(13)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Title_Body</th>\n",
       "      <th>ImageURLs</th>\n",
       "      <th>llm_zero_shot_title</th>\n",
       "      <th>llm_zero_shot_body</th>\n",
       "      <th>llm_zero_shot_combined</th>\n",
       "      <th>llm_few_shot_title</th>\n",
       "      <th>llm_few_shot_body</th>\n",
       "      <th>llm_few_shot_combined</th>\n",
       "      <th>llm_cot_title</th>\n",
       "      <th>llm_cot_body</th>\n",
       "      <th>llm_cot_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79041624</td>\n",
       "      <td>How to use switch branch button only seems to ...</td>\n",
       "      <td>I am used to looking at the source control che...</td>\n",
       "      <td>&lt;&lt;How to use switch branch button only seems t...</td>\n",
       "      <td>['https://code.visualstudio.com/assets/docs/ed...</td>\n",
       "      <td>Issue with Asynchronous Behavior in Node.js Fu...</td>\n",
       "      <td>I'm working on a Node.js project and encounter...</td>\n",
       "      <td>&lt;&lt;Issue with Asynchronous Behavior in Node.js ...</td>\n",
       "      <td>How to Properly Use 'require' in Node.js for M...</td>\n",
       "      <td>I'm working on a Node.js project and using the...</td>\n",
       "      <td>&lt;&lt;How to Properly Use 'require' in Node.js for...</td>\n",
       "      <td>How to Properly Use Promises in Node.js for Fi...</td>\n",
       "      <td>I'm working on a Node.js project using Visual ...</td>\n",
       "      <td>&lt;&lt;How to Properly Use Promises in Node.js for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79078823</td>\n",
       "      <td>LangGraph Error - Invalid Tool Calls when usin...</td>\n",
       "      <td>I'm following the LangGraph tutorial (from Lan...</td>\n",
       "      <td>&lt;&lt;LangGraph Error - Invalid Tool Calls when us...</td>\n",
       "      <td>['https://i.sstatic.net/lGCQGu49.png']</td>\n",
       "      <td>JSONDecodeError When Using Local Tool in Node....</td>\n",
       "      <td>I'm encountering an issue with a Node.js scrip...</td>\n",
       "      <td>&lt;&lt;JSONDecodeError When Using Local Tool in Nod...</td>\n",
       "      <td>JSONDecodeError when using local tool in Node.js</td>\n",
       "      <td>I'm trying to perform a simple arithmetic oper...</td>\n",
       "      <td>&lt;&lt;JSONDecodeError when using local tool in Nod...</td>\n",
       "      <td>How to Resolve JSONDecodeError in Node.js When...</td>\n",
       "      <td>I'm trying to perform a simple arithmetic oper...</td>\n",
       "      <td>&lt;&lt;How to Resolve JSONDecodeError in Node.js Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79120816</td>\n",
       "      <td>Why Is Intellesense &amp; Colour Formatting Not Wo...</td>\n",
       "      <td>Intellisense and text colour formatting is not...</td>\n",
       "      <td>&lt;&lt;Why Is Intellesense &amp; Colour Formatting Not ...</td>\n",
       "      <td>['https://i.sstatic.net/9SW9hTKN.png']</td>\n",
       "      <td>Nullable DateTime Property Causing Issues in A...</td>\n",
       "      <td>I'm working on an ASP.NET project and have def...</td>\n",
       "      <td>&lt;&lt;Nullable DateTime Property Causing Issues in...</td>\n",
       "      <td>Nullable DateTime Property in C# Model Class</td>\n",
       "      <td>I am working on a C# project and have a model ...</td>\n",
       "      <td>&lt;&lt;Nullable DateTime Property in C# Model Class...</td>\n",
       "      <td>How to handle nullable DateTime properties in ...</td>\n",
       "      <td>I'm working on a C# project and have a model c...</td>\n",
       "      <td>&lt;&lt;How to handle nullable DateTime properties i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79041900</td>\n",
       "      <td>Http 429 is not returnable in status code from...</td>\n",
       "      <td>[Route(\"{route}\")]\\n[HttpPost]\\npublic async T...</td>\n",
       "      <td>&lt;&lt;Http 429 is not returnable in status code fr...</td>\n",
       "      <td>['https://i.sstatic.net/jtsyQ5SF.png']</td>\n",
       "      <td>Why am I receiving a 429 \"Too Many Requests\" e...</td>\n",
       "      <td>I'm working on an API integration using a REST...</td>\n",
       "      <td>&lt;&lt;Why am I receiving a 429 \"Too Many Requests\"...</td>\n",
       "      <td>Handling 429 Too Many Requests Error Despite 2...</td>\n",
       "      <td>I'm encountering an issue where my HTTP reques...</td>\n",
       "      <td>&lt;&lt;Handling 429 Too Many Requests Error Despite...</td>\n",
       "      <td>Why am I receiving a 429 \"Too Many Requests\" e...</td>\n",
       "      <td>I'm using Postman to test an API, and I'm enco...</td>\n",
       "      <td>&lt;&lt;Why am I receiving a 429 \"Too Many Requests\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79081114</td>\n",
       "      <td>PhpStorm is marking variables given by Control...</td>\n",
       "      <td>PhpStorm is marking variables given by Control...</td>\n",
       "      <td>&lt;&lt;PhpStorm is marking variables given by Contr...</td>\n",
       "      <td>['https://i.sstatic.net/AFTN318J.png']</td>\n",
       "      <td>Yii2 Password Input Field Not Rendering Correctly</td>\n",
       "      <td>I'm working on a Yii2 project and trying to re...</td>\n",
       "      <td>&lt;&lt;Yii2 Password Input Field Not Rendering Corr...</td>\n",
       "      <td>PHP Error: Undefined Variable in Form Field</td>\n",
       "      <td>I'm encountering an 'undefined variable' error...</td>\n",
       "      <td>&lt;&lt;PHP Error: Undefined Variable in Form Field&gt;...</td>\n",
       "      <td>How to resolve \"undefined variable\" error in P...</td>\n",
       "      <td>I'm working on a PHP project using a form help...</td>\n",
       "      <td>&lt;&lt;How to resolve \"undefined variable\" error in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  79041624  How to use switch branch button only seems to ...   \n",
       "1  79078823  LangGraph Error - Invalid Tool Calls when usin...   \n",
       "2  79120816  Why Is Intellesense & Colour Formatting Not Wo...   \n",
       "3  79041900  Http 429 is not returnable in status code from...   \n",
       "4  79081114  PhpStorm is marking variables given by Control...   \n",
       "\n",
       "                                                Body  \\\n",
       "0  I am used to looking at the source control che...   \n",
       "1  I'm following the LangGraph tutorial (from Lan...   \n",
       "2  Intellisense and text colour formatting is not...   \n",
       "3  [Route(\"{route}\")]\\n[HttpPost]\\npublic async T...   \n",
       "4  PhpStorm is marking variables given by Control...   \n",
       "\n",
       "                                          Title_Body  \\\n",
       "0  <<How to use switch branch button only seems t...   \n",
       "1  <<LangGraph Error - Invalid Tool Calls when us...   \n",
       "2  <<Why Is Intellesense & Colour Formatting Not ...   \n",
       "3  <<Http 429 is not returnable in status code fr...   \n",
       "4  <<PhpStorm is marking variables given by Contr...   \n",
       "\n",
       "                                           ImageURLs  \\\n",
       "0  ['https://code.visualstudio.com/assets/docs/ed...   \n",
       "1             ['https://i.sstatic.net/lGCQGu49.png']   \n",
       "2             ['https://i.sstatic.net/9SW9hTKN.png']   \n",
       "3             ['https://i.sstatic.net/jtsyQ5SF.png']   \n",
       "4             ['https://i.sstatic.net/AFTN318J.png']   \n",
       "\n",
       "                                 llm_zero_shot_title  \\\n",
       "0  Issue with Asynchronous Behavior in Node.js Fu...   \n",
       "1  JSONDecodeError When Using Local Tool in Node....   \n",
       "2  Nullable DateTime Property Causing Issues in A...   \n",
       "3  Why am I receiving a 429 \"Too Many Requests\" e...   \n",
       "4  Yii2 Password Input Field Not Rendering Correctly   \n",
       "\n",
       "                                  llm_zero_shot_body  \\\n",
       "0  I'm working on a Node.js project and encounter...   \n",
       "1  I'm encountering an issue with a Node.js scrip...   \n",
       "2  I'm working on an ASP.NET project and have def...   \n",
       "3  I'm working on an API integration using a REST...   \n",
       "4  I'm working on a Yii2 project and trying to re...   \n",
       "\n",
       "                              llm_zero_shot_combined  \\\n",
       "0  <<Issue with Asynchronous Behavior in Node.js ...   \n",
       "1  <<JSONDecodeError When Using Local Tool in Nod...   \n",
       "2  <<Nullable DateTime Property Causing Issues in...   \n",
       "3  <<Why am I receiving a 429 \"Too Many Requests\"...   \n",
       "4  <<Yii2 Password Input Field Not Rendering Corr...   \n",
       "\n",
       "                                  llm_few_shot_title  \\\n",
       "0  How to Properly Use 'require' in Node.js for M...   \n",
       "1   JSONDecodeError when using local tool in Node.js   \n",
       "2       Nullable DateTime Property in C# Model Class   \n",
       "3  Handling 429 Too Many Requests Error Despite 2...   \n",
       "4        PHP Error: Undefined Variable in Form Field   \n",
       "\n",
       "                                   llm_few_shot_body  \\\n",
       "0  I'm working on a Node.js project and using the...   \n",
       "1  I'm trying to perform a simple arithmetic oper...   \n",
       "2  I am working on a C# project and have a model ...   \n",
       "3  I'm encountering an issue where my HTTP reques...   \n",
       "4  I'm encountering an 'undefined variable' error...   \n",
       "\n",
       "                               llm_few_shot_combined  \\\n",
       "0  <<How to Properly Use 'require' in Node.js for...   \n",
       "1  <<JSONDecodeError when using local tool in Nod...   \n",
       "2  <<Nullable DateTime Property in C# Model Class...   \n",
       "3  <<Handling 429 Too Many Requests Error Despite...   \n",
       "4  <<PHP Error: Undefined Variable in Form Field>...   \n",
       "\n",
       "                                       llm_cot_title  \\\n",
       "0  How to Properly Use Promises in Node.js for Fi...   \n",
       "1  How to Resolve JSONDecodeError in Node.js When...   \n",
       "2  How to handle nullable DateTime properties in ...   \n",
       "3  Why am I receiving a 429 \"Too Many Requests\" e...   \n",
       "4  How to resolve \"undefined variable\" error in P...   \n",
       "\n",
       "                                        llm_cot_body  \\\n",
       "0  I'm working on a Node.js project using Visual ...   \n",
       "1  I'm trying to perform a simple arithmetic oper...   \n",
       "2  I'm working on a C# project and have a model c...   \n",
       "3  I'm using Postman to test an API, and I'm enco...   \n",
       "4  I'm working on a PHP project using a form help...   \n",
       "\n",
       "                                    llm_cot_combined  \n",
       "0  <<How to Properly Use Promises in Node.js for ...  \n",
       "1  <<How to Resolve JSONDecodeError in Node.js Wh...  \n",
       "2  <<How to handle nullable DateTime properties i...  \n",
       "3  <<Why am I receiving a 429 \"Too Many Requests\"...  \n",
       "4  <<How to resolve \"undefined variable\" error in...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for original content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c567e8a2964bd1b1ea83246081cbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0270a1db70dc44bfae1d7d9a8fcb4f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d493f32c8254c53a062bb22683eb0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for LLM responses...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090570d6d0ce4a6bba9bf9f144135678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f6c937e3734f55922d2143294ae277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92a0235db71401e971a88ae066c46f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95dfb501fec6494695d82e7680eea7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283d52b963f248d3b5c3bb2e4bd12555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48738789470246d4b1aad9c919d5efee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43cdeb09d844a05a53025eca0867376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3382a28a364269b5c11716d1c2d4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f10ef58d6f543bc940b01bd5e7189c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TITLE Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4350\n",
      "median: 0.4398\n",
      "std: 0.2173\n",
      "min: -0.0140\n",
      "max: 0.8970\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 2.80%\n",
      "High: 23.78%\n",
      "Moderate: 31.47%\n",
      "Low: 25.17%\n",
      "Very Low: 16.08%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4248\n",
      "median: 0.4154\n",
      "std: 0.2217\n",
      "min: -0.0189\n",
      "max: 0.9739\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 5.59%\n",
      "High: 16.78%\n",
      "Moderate: 30.77%\n",
      "Low: 28.67%\n",
      "Very Low: 17.48%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4370\n",
      "median: 0.4300\n",
      "std: 0.2252\n",
      "min: -0.0189\n",
      "max: 0.9138\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 4.90%\n",
      "High: 23.08%\n",
      "Moderate: 29.37%\n",
      "Low: 23.78%\n",
      "Very Low: 18.18%\n",
      "\n",
      "BODY Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5046\n",
      "median: 0.5201\n",
      "std: 0.2097\n",
      "min: -0.0391\n",
      "max: 0.8924\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 4.90%\n",
      "High: 32.87%\n",
      "Moderate: 31.47%\n",
      "Low: 22.38%\n",
      "Very Low: 7.69%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4724\n",
      "median: 0.4682\n",
      "std: 0.2043\n",
      "min: -0.0369\n",
      "max: 0.8933\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 5.59%\n",
      "High: 24.48%\n",
      "Moderate: 34.27%\n",
      "Low: 25.87%\n",
      "Very Low: 9.09%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4946\n",
      "median: 0.5216\n",
      "std: 0.2049\n",
      "min: -0.0439\n",
      "max: 0.8521\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 6.29%\n",
      "High: 27.27%\n",
      "Moderate: 34.97%\n",
      "Low: 18.88%\n",
      "Very Low: 11.89%\n",
      "\n",
      "COMBINED Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5587\n",
      "median: 0.6051\n",
      "std: 0.2118\n",
      "min: -0.0704\n",
      "max: 0.9160\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 12.59%\n",
      "High: 38.46%\n",
      "Moderate: 24.48%\n",
      "Low: 17.48%\n",
      "Very Low: 6.29%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5367\n",
      "median: 0.5658\n",
      "std: 0.2053\n",
      "min: 0.0231\n",
      "max: 0.9010\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 9.09%\n",
      "High: 36.36%\n",
      "Moderate: 27.97%\n",
      "Low: 17.48%\n",
      "Very Low: 9.09%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5521\n",
      "median: 0.6015\n",
      "std: 0.2084\n",
      "min: 0.0386\n",
      "max: 0.8981\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 11.19%\n",
      "High: 39.86%\n",
      "Moderate: 24.48%\n",
      "Low: 17.48%\n",
      "Very Low: 6.99%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_embeddings_and_analyze(df, model_name='sentence-transformers/all-mpnet-base-v1'):\n",
    "    \"\"\"\n",
    "    Create embeddings using SentenceTransformer and analyze similarities for both\n",
    "    separate title/body responses and combined responses\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings for original content\n",
    "    print(\"Generating embeddings for original content...\")\n",
    "    title_embeddings = model.encode(df['Title'].tolist(), show_progress_bar=True)\n",
    "    body_embeddings = model.encode(df['Body'].tolist(), show_progress_bar=True)\n",
    "    title_body_embeddings = model.encode(df['Title_Body'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    # Generate embeddings for LLM responses\n",
    "    print(\"\\nGenerating embeddings for LLM responses...\")\n",
    "    response_embeddings = {\n",
    "        # Title responses\n",
    "        'zero_shot_title': model.encode(df['llm_zero_shot_title'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_title': model.encode(df['llm_few_shot_title'].tolist(), show_progress_bar=True),\n",
    "        'cot_title': model.encode(df['llm_cot_title'].tolist(), show_progress_bar=True),\n",
    "        \n",
    "        # Body responses\n",
    "        'zero_shot_body': model.encode(df['llm_zero_shot_body'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_body': model.encode(df['llm_few_shot_body'].tolist(), show_progress_bar=True),\n",
    "        'cot_body': model.encode(df['llm_cot_body'].tolist(), show_progress_bar=True),\n",
    "        \n",
    "        # Combined responses\n",
    "        'zero_shot_combined': model.encode(df['llm_zero_shot_combined'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_combined': model.encode(df['llm_few_shot_combined'].tolist(), show_progress_bar=True),\n",
    "        'cot_combined': model.encode(df['llm_cot_combined'].tolist(), show_progress_bar=True)\n",
    "    }\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate title similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['title'][response_type] = np.diagonal(\n",
    "            cosine_similarity(title_embeddings, response_embeddings[f'{response_type}_title'])\n",
    "        )\n",
    "    \n",
    "    # Calculate body similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['body'][response_type] = np.diagonal(\n",
    "            cosine_similarity(body_embeddings, response_embeddings[f'{response_type}_body'])\n",
    "        )\n",
    "    \n",
    "    # Calculate combined similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['combined'][response_type] = np.diagonal(\n",
    "            cosine_similarity(title_body_embeddings, response_embeddings[f'{response_type}_combined'])\n",
    "        )\n",
    "    \n",
    "    # Add similarity scores to dataframe\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        df[f'similarity_{response_type}_title'] = similarities['title'][response_type]\n",
    "        df[f'similarity_{response_type}_body'] = similarities['body'][response_type]\n",
    "        df[f'similarity_{response_type}_combined'] = similarities['combined'][response_type]\n",
    "    \n",
    "    # Save embeddings\n",
    "    np.save('Data/title_embeddings_st.npy', title_embeddings)\n",
    "    np.save('Data/body_embeddings_st.npy', body_embeddings)\n",
    "    np.save('Data/title_body_embeddings_st.npy', title_body_embeddings)\n",
    "    \n",
    "    for response_type, embeddings in response_embeddings.items():\n",
    "        np.save(f'Data/{response_type}_embeddings_st.npy', embeddings)\n",
    "    \n",
    "    return df, similarities\n",
    "\n",
    "def analyze_similarities(similarities):\n",
    "    \"\"\"\n",
    "    Analyze and visualize similarity distributions for title, body, and combined responses\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        'Very High': (0.8, 1.0),\n",
    "        'High': (0.6, 0.8),\n",
    "        'Moderate': (0.4, 0.6),\n",
    "        'Low': (0.2, 0.4),\n",
    "        'Very Low': (0.0, 0.2)\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Create figures for each type of response\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle(f'Similarity Analysis for {response_category.capitalize()} Responses')\n",
    "        \n",
    "        # Plot distributions and calculate statistics for each LLM type\n",
    "        for idx, response_type in enumerate(['zero_shot', 'few_shot', 'cot']):\n",
    "            scores = similarities[response_category][response_type]\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = {\n",
    "                'mean': np.mean(scores),\n",
    "                'median': np.median(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "            \n",
    "            # Calculate distribution across categories\n",
    "            distribution = {}\n",
    "            for category, (low, high) in categories.items():\n",
    "                count = np.sum((scores >= low) & (scores < high))\n",
    "                percentage = (count / len(scores)) * 100\n",
    "                distribution[category] = percentage\n",
    "                \n",
    "            results[response_category][response_type] = {\n",
    "                'statistics': stats,\n",
    "                'distribution': distribution\n",
    "            }\n",
    "            \n",
    "            # Plot histogram\n",
    "            ax = axes[idx]\n",
    "            sns.histplot(scores, bins=30, ax=ax)\n",
    "            ax.set_title(f'{response_type.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel('Similarity Score')\n",
    "            ax.set_ylabel('Count')\n",
    "            \n",
    "            # Add category boundaries\n",
    "            for category, (low, high) in categories.items():\n",
    "                if low > 0:  # Don't plot the lowest boundary\n",
    "                    ax.axvline(x=low, color='r', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'Data/similarity_distributions_{response_category}_st.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Print analysis\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        print(f\"\\n{response_category.upper()} Response Analysis:\")\n",
    "        for response_type, result in results[response_category].items():\n",
    "            print(f\"\\n{response_type.upper()}:\")\n",
    "            print(\"\\nBasic Statistics:\")\n",
    "            for metric, value in result['statistics'].items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            \n",
    "            print(\"\\nDistribution across categories:\")\n",
    "            for category, percentage in result['distribution'].items():\n",
    "                print(f\"{category}: {percentage:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the combined CSV file\n",
    "    df = pd.read_csv('Data/GPT-4o/GPT-4o_responses_combined.csv')\n",
    "    \n",
    "    # Generate embeddings and calculate similarities\n",
    "    df_with_similarities, similarities = create_embeddings_and_analyze(df)\n",
    "    \n",
    "    # Analyze and visualize results\n",
    "    analysis_results = analyze_similarities(similarities)\n",
    "    \n",
    "    # Save updated dataframe with similarities\n",
    "    df_with_similarities.to_csv('Data/GPT-4o/GPT-4o_responses_with_similarities_st.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for original content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1f8cf84eb34172a8633ba7b89d2a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732689b2a01540c1a468c0aaade764da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389613e8f9444798bcce0322de7578c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for LLM responses...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a3a949f036439c9a0ba8c6fd9154a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7638cc421d47d4a4c1c2e3d300c991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa39a92dbcd94ecfbb115dabee5d2a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c19a459b85a413294df920f58dbaf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f331824182b4ffa8995f0a97b5bb22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc5d12749ab4d70a107ab51ed80f922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbc30e4f90c4eb593126c71dc643820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b168cd67a4514ecea5fd3d864dcc412e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ea890682bb4ab98698b0e3dbbd7bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TITLE Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4561\n",
      "median: 0.4492\n",
      "std: 0.2215\n",
      "min: -0.0331\n",
      "max: 0.9435\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 6.29%\n",
      "High: 26.57%\n",
      "Moderate: 25.17%\n",
      "Low: 27.27%\n",
      "Very Low: 13.99%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4453\n",
      "median: 0.4525\n",
      "std: 0.2297\n",
      "min: -0.0264\n",
      "max: 0.9809\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 6.29%\n",
      "High: 19.58%\n",
      "Moderate: 34.27%\n",
      "Low: 21.68%\n",
      "Very Low: 16.78%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4539\n",
      "median: 0.4557\n",
      "std: 0.2376\n",
      "min: -0.0296\n",
      "max: 0.9612\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 9.09%\n",
      "High: 20.98%\n",
      "Moderate: 27.97%\n",
      "Low: 24.48%\n",
      "Very Low: 16.78%\n",
      "\n",
      "BODY Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5215\n",
      "median: 0.5378\n",
      "std: 0.1935\n",
      "min: -0.0131\n",
      "max: 0.9503\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 7.69%\n",
      "High: 30.07%\n",
      "Moderate: 37.06%\n",
      "Low: 18.88%\n",
      "Very Low: 5.59%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4907\n",
      "median: 0.4961\n",
      "std: 0.1933\n",
      "min: -0.0568\n",
      "max: 0.9164\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 4.20%\n",
      "High: 25.87%\n",
      "Moderate: 39.16%\n",
      "Low: 23.08%\n",
      "Very Low: 6.29%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5155\n",
      "median: 0.5281\n",
      "std: 0.1935\n",
      "min: -0.0081\n",
      "max: 0.9496\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 6.99%\n",
      "High: 32.87%\n",
      "Moderate: 34.97%\n",
      "Low: 19.58%\n",
      "Very Low: 4.90%\n",
      "\n",
      "COMBINED Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5862\n",
      "median: 0.6097\n",
      "std: 0.1906\n",
      "min: 0.0025\n",
      "max: 0.9582\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 10.49%\n",
      "High: 41.26%\n",
      "Moderate: 32.17%\n",
      "Low: 11.89%\n",
      "Very Low: 4.20%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5616\n",
      "median: 0.5786\n",
      "std: 0.1908\n",
      "min: -0.0199\n",
      "max: 0.9418\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 9.79%\n",
      "High: 34.27%\n",
      "Moderate: 37.06%\n",
      "Low: 14.69%\n",
      "Very Low: 3.50%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5766\n",
      "median: 0.5986\n",
      "std: 0.1891\n",
      "min: 0.0168\n",
      "max: 0.9343\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 9.09%\n",
      "High: 40.56%\n",
      "Moderate: 34.97%\n",
      "Low: 9.79%\n",
      "Very Low: 5.59%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_embeddings_and_analyze(df, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Create embeddings using SentenceTransformer and analyze similarities for both\n",
    "    separate title/body responses and combined responses\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings for original content\n",
    "    print(\"Generating embeddings for original content...\")\n",
    "    title_embeddings = model.encode(df['Title'].tolist(), show_progress_bar=True)\n",
    "    body_embeddings = model.encode(df['Body'].tolist(), show_progress_bar=True)\n",
    "    title_body_embeddings = model.encode(df['Title_Body'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    # Generate embeddings for LLM responses\n",
    "    print(\"\\nGenerating embeddings for LLM responses...\")\n",
    "    response_embeddings = {\n",
    "        # Title responses\n",
    "        'zero_shot_title': model.encode(df['llm_zero_shot_title'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_title': model.encode(df['llm_few_shot_title'].tolist(), show_progress_bar=True),\n",
    "        'cot_title': model.encode(df['llm_cot_title'].tolist(), show_progress_bar=True),\n",
    "        \n",
    "        # Body responses\n",
    "        'zero_shot_body': model.encode(df['llm_zero_shot_body'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_body': model.encode(df['llm_few_shot_body'].tolist(), show_progress_bar=True),\n",
    "        'cot_body': model.encode(df['llm_cot_body'].tolist(), show_progress_bar=True),\n",
    "        \n",
    "        # Combined responses\n",
    "        'zero_shot_combined': model.encode(df['llm_zero_shot_combined'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_combined': model.encode(df['llm_few_shot_combined'].tolist(), show_progress_bar=True),\n",
    "        'cot_combined': model.encode(df['llm_cot_combined'].tolist(), show_progress_bar=True)\n",
    "    }\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate title similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['title'][response_type] = np.diagonal(\n",
    "            cosine_similarity(title_embeddings, response_embeddings[f'{response_type}_title'])\n",
    "        )\n",
    "    \n",
    "    # Calculate body similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['body'][response_type] = np.diagonal(\n",
    "            cosine_similarity(body_embeddings, response_embeddings[f'{response_type}_body'])\n",
    "        )\n",
    "    \n",
    "    # Calculate combined similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['combined'][response_type] = np.diagonal(\n",
    "            cosine_similarity(title_body_embeddings, response_embeddings[f'{response_type}_combined'])\n",
    "        )\n",
    "    \n",
    "    # Add similarity scores to dataframe\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        df[f'similarity_{response_type}_title'] = similarities['title'][response_type]\n",
    "        df[f'similarity_{response_type}_body'] = similarities['body'][response_type]\n",
    "        df[f'similarity_{response_type}_combined'] = similarities['combined'][response_type]\n",
    "    \n",
    "    # Save embeddings\n",
    "    np.save('Data/title_embeddings_st.npy', title_embeddings)\n",
    "    np.save('Data/body_embeddings_st.npy', body_embeddings)\n",
    "    np.save('Data/title_body_embeddings_st.npy', title_body_embeddings)\n",
    "    \n",
    "    for response_type, embeddings in response_embeddings.items():\n",
    "        np.save(f'Data/{response_type}_embeddings_st.npy', embeddings)\n",
    "    \n",
    "    return df, similarities\n",
    "\n",
    "def analyze_similarities(similarities):\n",
    "    \"\"\"\n",
    "    Analyze and visualize similarity distributions for title, body, and combined responses\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        'Very High': (0.8, 1.0),\n",
    "        'High': (0.6, 0.8),\n",
    "        'Moderate': (0.4, 0.6),\n",
    "        'Low': (0.2, 0.4),\n",
    "        'Very Low': (0.0, 0.2)\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Create figures for each type of response\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle(f'Similarity Analysis for {response_category.capitalize()} Responses')\n",
    "        \n",
    "        # Plot distributions and calculate statistics for each LLM type\n",
    "        for idx, response_type in enumerate(['zero_shot', 'few_shot', 'cot']):\n",
    "            scores = similarities[response_category][response_type]\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = {\n",
    "                'mean': np.mean(scores),\n",
    "                'median': np.median(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "            \n",
    "            # Calculate distribution across categories\n",
    "            distribution = {}\n",
    "            for category, (low, high) in categories.items():\n",
    "                count = np.sum((scores >= low) & (scores < high))\n",
    "                percentage = (count / len(scores)) * 100\n",
    "                distribution[category] = percentage\n",
    "                \n",
    "            results[response_category][response_type] = {\n",
    "                'statistics': stats,\n",
    "                'distribution': distribution\n",
    "            }\n",
    "            \n",
    "            # Plot histogram\n",
    "            ax = axes[idx]\n",
    "            sns.histplot(scores, bins=30, ax=ax)\n",
    "            ax.set_title(f'{response_type.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel('Similarity Score')\n",
    "            ax.set_ylabel('Count')\n",
    "            \n",
    "            # Add category boundaries\n",
    "            for category, (low, high) in categories.items():\n",
    "                if low > 0:  # Don't plot the lowest boundary\n",
    "                    ax.axvline(x=low, color='r', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'Data/similarity_distributions_{response_category}_st.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Print analysis\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        print(f\"\\n{response_category.upper()} Response Analysis:\")\n",
    "        for response_type, result in results[response_category].items():\n",
    "            print(f\"\\n{response_type.upper()}:\")\n",
    "            print(\"\\nBasic Statistics:\")\n",
    "            for metric, value in result['statistics'].items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            \n",
    "            print(\"\\nDistribution across categories:\")\n",
    "            for category, percentage in result['distribution'].items():\n",
    "                print(f\"{category}: {percentage:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the combined CSV file\n",
    "    df = pd.read_csv('Data/GPT-4o/GPT-4o_responses_combined.csv')\n",
    "    \n",
    "    # Generate embeddings and calculate similarities\n",
    "    df_with_similarities, similarities = create_embeddings_and_analyze(df)\n",
    "    \n",
    "    # Analyze and visualize results\n",
    "    analysis_results = analyze_similarities(similarities)\n",
    "    \n",
    "    # Save updated dataframe with similarities\n",
    "    df_with_similarities.to_csv('Data/GPT-4o/GPT-4o_responses_with_similarities_st.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for original content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270dc1078f624e5bbb95a37fc2182246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e23896fb4fc47bbac56e728d4d36fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8473c0c8e14522a66dbb052193bef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for LLM responses...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3a887eae304961bc4fdf902d489b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397e9179f0e640509bf0b431e9644d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25aa1644b124411bb93757f67bc22c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795c1b28d16a4320a6a1bf678924dc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9533dc8eb44cc9be10283b67ebe8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c8fc1538cd455cb2a7d6355de9e835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be0c69a687f4febb2ff45a1c9603130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2eac949096941ef890a4105b39d66eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63abb42d15248a08784f265dec83a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TITLE Response Analysis (KartonBERT):\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5971\n",
      "median: 0.5963\n",
      "std: 0.1495\n",
      "min: 0.2466\n",
      "max: 0.9380\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 8.39%\n",
      "High: 41.26%\n",
      "Moderate: 39.86%\n",
      "Low: 10.49%\n",
      "Very Low: 0.00%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5879\n",
      "median: 0.5895\n",
      "std: 0.1542\n",
      "min: 0.2504\n",
      "max: 0.9715\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 9.79%\n",
      "High: 36.36%\n",
      "Moderate: 40.56%\n",
      "Low: 13.29%\n",
      "Very Low: 0.00%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5909\n",
      "median: 0.6016\n",
      "std: 0.1585\n",
      "min: 0.2224\n",
      "max: 0.9297\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 8.39%\n",
      "High: 43.36%\n",
      "Moderate: 37.06%\n",
      "Low: 11.19%\n",
      "Very Low: 0.00%\n",
      "\n",
      "BODY Response Analysis (KartonBERT):\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.7142\n",
      "median: 0.7265\n",
      "std: 0.1182\n",
      "min: 0.2825\n",
      "max: 0.9454\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 21.68%\n",
      "High: 61.54%\n",
      "Moderate: 15.38%\n",
      "Low: 1.40%\n",
      "Very Low: 0.00%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.6974\n",
      "median: 0.7024\n",
      "std: 0.1097\n",
      "min: 0.3284\n",
      "max: 0.9280\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 17.48%\n",
      "High: 65.03%\n",
      "Moderate: 16.78%\n",
      "Low: 0.70%\n",
      "Very Low: 0.00%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.7087\n",
      "median: 0.7209\n",
      "std: 0.1079\n",
      "min: 0.3379\n",
      "max: 0.9383\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 18.88%\n",
      "High: 66.43%\n",
      "Moderate: 13.99%\n",
      "Low: 0.70%\n",
      "Very Low: 0.00%\n",
      "\n",
      "COMBINED Response Analysis (KartonBERT):\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.7215\n",
      "median: 0.7419\n",
      "std: 0.1262\n",
      "min: 0.2900\n",
      "max: 0.9587\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 28.67%\n",
      "High: 56.64%\n",
      "Moderate: 13.29%\n",
      "Low: 1.40%\n",
      "Very Low: 0.00%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.7036\n",
      "median: 0.7117\n",
      "std: 0.1213\n",
      "min: 0.3387\n",
      "max: 0.9390\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 22.38%\n",
      "High: 58.74%\n",
      "Moderate: 18.18%\n",
      "Low: 0.70%\n",
      "Very Low: 0.00%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.7123\n",
      "median: 0.7306\n",
      "std: 0.1193\n",
      "min: 0.3150\n",
      "max: 0.9424\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 21.68%\n",
      "High: 61.54%\n",
      "Moderate: 15.38%\n",
      "Low: 1.40%\n",
      "Very Low: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_embeddings_and_analyze(df, model_name='OrlikB/KartonBERT-USE-base-v1'):\n",
    "    \"\"\"\n",
    "    Create embeddings using KartonBERT and analyze similarities with normalized embeddings\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings for original content\n",
    "    print(\"Generating embeddings for original content...\")\n",
    "    title_embeddings = model.encode(df['Title'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    body_embeddings = model.encode(df['Body'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    title_body_embeddings = model.encode(df['Title_Body'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    \n",
    "    # Generate embeddings for LLM responses\n",
    "    print(\"\\nGenerating embeddings for LLM responses...\")\n",
    "    response_embeddings = {\n",
    "        # Title responses\n",
    "        'zero_shot_title': model.encode(df['llm_zero_shot_title'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'few_shot_title': model.encode(df['llm_few_shot_title'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'cot_title': model.encode(df['llm_cot_title'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        \n",
    "        # Body responses\n",
    "        'zero_shot_body': model.encode(df['llm_zero_shot_body'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'few_shot_body': model.encode(df['llm_few_shot_body'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'cot_body': model.encode(df['llm_cot_body'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        \n",
    "        # Combined responses\n",
    "        'zero_shot_combined': model.encode(df['llm_zero_shot_combined'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'few_shot_combined': model.encode(df['llm_few_shot_combined'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'cot_combined': model.encode(df['llm_cot_combined'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    }\n",
    "    \n",
    "    # Calculate similarities using dot product\n",
    "    similarities = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate title similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        # Using dot product for normalized vectors\n",
    "        similarities['title'][response_type] = np.sum(\n",
    "            title_embeddings * response_embeddings[f'{response_type}_title'], axis=1\n",
    "        )\n",
    "    \n",
    "    # Calculate body similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['body'][response_type] = np.sum(\n",
    "            body_embeddings * response_embeddings[f'{response_type}_body'], axis=1\n",
    "        )\n",
    "    \n",
    "    # Calculate combined similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['combined'][response_type] = np.sum(\n",
    "            title_body_embeddings * response_embeddings[f'{response_type}_combined'], axis=1\n",
    "        )\n",
    "    \n",
    "    # Add similarity scores to dataframe\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        df[f'similarity_{response_type}_title'] = similarities['title'][response_type]\n",
    "        df[f'similarity_{response_type}_body'] = similarities['body'][response_type]\n",
    "        df[f'similarity_{response_type}_combined'] = similarities['combined'][response_type]\n",
    "    \n",
    "    # Save embeddings\n",
    "    np.save('Data/title_embeddings_kartonbert.npy', title_embeddings)\n",
    "    np.save('Data/body_embeddings_kartonbert.npy', body_embeddings)\n",
    "    np.save('Data/title_body_embeddings_kartonbert.npy', title_body_embeddings)\n",
    "    \n",
    "    for response_type, embeddings in response_embeddings.items():\n",
    "        np.save(f'Data/{response_type}_embeddings_kartonbert.npy', embeddings)\n",
    "    \n",
    "    return df, similarities\n",
    "\n",
    "def analyze_similarities(similarities):\n",
    "    \"\"\"\n",
    "    Analyze and visualize similarity distributions for title, body, and combined responses\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        'Very High': (0.8, 1.0),\n",
    "        'High': (0.6, 0.8),\n",
    "        'Moderate': (0.4, 0.6),\n",
    "        'Low': (0.2, 0.4),\n",
    "        'Very Low': (0.0, 0.2)\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Create figures for each type of response\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle(f'Similarity Analysis for {response_category.capitalize()} Responses (KartonBERT)')\n",
    "        \n",
    "        # Plot distributions and calculate statistics for each LLM type\n",
    "        for idx, response_type in enumerate(['zero_shot', 'few_shot', 'cot']):\n",
    "            scores = similarities[response_category][response_type]\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = {\n",
    "                'mean': np.mean(scores),\n",
    "                'median': np.median(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "            \n",
    "            # Calculate distribution across categories\n",
    "            distribution = {}\n",
    "            for category, (low, high) in categories.items():\n",
    "                count = np.sum((scores >= low) & (scores < high))\n",
    "                percentage = (count / len(scores)) * 100\n",
    "                distribution[category] = percentage\n",
    "                \n",
    "            results[response_category][response_type] = {\n",
    "                'statistics': stats,\n",
    "                'distribution': distribution\n",
    "            }\n",
    "            \n",
    "            # Plot histogram\n",
    "            ax = axes[idx]\n",
    "            sns.histplot(scores, bins=30, ax=ax)\n",
    "            ax.set_title(f'{response_type.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel('Similarity Score')\n",
    "            ax.set_ylabel('Count')\n",
    "            \n",
    "            # Add category boundaries\n",
    "            for category, (low, high) in categories.items():\n",
    "                if low > 0:  # Don't plot the lowest boundary\n",
    "                    ax.axvline(x=low, color='r', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'Data/similarity_distributions_{response_category}_kartonbert.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Print analysis\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        print(f\"\\n{response_category.upper()} Response Analysis (KartonBERT):\")\n",
    "        for response_type, result in results[response_category].items():\n",
    "            print(f\"\\n{response_type.upper()}:\")\n",
    "            print(\"\\nBasic Statistics:\")\n",
    "            for metric, value in result['statistics'].items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            \n",
    "            print(\"\\nDistribution across categories:\")\n",
    "            for category, percentage in result['distribution'].items():\n",
    "                print(f\"{category}: {percentage:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the combined CSV file\n",
    "    df = pd.read_csv('Data/GPT-4o/GPT-4o_responses_combined.csv')\n",
    "    \n",
    "    # Generate embeddings and calculate similarities\n",
    "    df_with_similarities, similarities = create_embeddings_and_analyze(df)\n",
    "    \n",
    "    # Analyze and visualize results\n",
    "    analysis_results = analyze_similarities(similarities)\n",
    "    \n",
    "    # Save updated dataframe with similarities\n",
    "    df_with_similarities.to_csv('Data/GPT-4o/GPT-4o_responses_with_similarities_st.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
