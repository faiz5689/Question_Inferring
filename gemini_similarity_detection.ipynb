{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file created with 143 rows\n",
      "\n",
      "Columns in the combined file:\n",
      "- Id\n",
      "- Title\n",
      "- Body\n",
      "- Title_Body\n",
      "- ImageURLs\n",
      "- llm_zero_shot_title\n",
      "- llm_zero_shot_body\n",
      "- llm_zero_shot_combined\n",
      "- llm_few_shot_title\n",
      "- llm_few_shot_body\n",
      "- llm_few_shot_combined\n",
      "- llm_cot_title\n",
      "- llm_cot_body\n",
      "- llm_cot_combined\n",
      "\n",
      "Sample of first row:\n",
      "\n",
      "Id:\n",
      "79146548\n",
      "\n",
      "Title:\n",
      "GitHub Copilot responds to 'Hey Code' but dictation doesn't work\n",
      "\n",
      "Body:\n",
      "As the title explains, I can start an inline chat session using the 'hey code' voice command in VS C...\n",
      "\n",
      "Title_Body:\n",
      "<<GitHub Copilot responds to 'Hey Code' but dictation doesn't work>>\n",
      "<<As the title explains, I can ...\n",
      "\n",
      "ImageURLs:\n",
      "['https://i.sstatic.net/MgGjdapB.png']\n",
      "\n",
      "llm_zero_shot_title:\n",
      "Why is my IDE/Code editor suggesting \"Ask Copilot\" when I have code on the next line?\n",
      "\n",
      "llm_zero_shot_body:\n",
      "I'm trying to use the \"Ask Copilot\" feature in my IDE, but it's not working as expected. When I clic...\n",
      "\n",
      "llm_zero_shot_combined:\n",
      "<<Why is my IDE/Code editor suggesting \"Ask Copilot\" when I have code on the next line?>>\n",
      "<<I'm tryi...\n",
      "\n",
      "llm_few_shot_title:\n",
      "How to Implement Drag and Drop Functionality Between Two Tree Views in WPF?\n",
      "\n",
      "llm_few_shot_body:\n",
      "I am trying to find a reference to the `SetupEventHandlers` method in my codebase, but when I perfor...\n",
      "\n",
      "llm_few_shot_combined:\n",
      "<<How to Implement Drag and Drop Functionality Between Two Tree Views in WPF?>>\n",
      "<<I am trying to fin...\n",
      "\n",
      "llm_cot_title:\n",
      "How to use GitHub Copilot to generate code for SetupEventHandlers() and InitializePalette() methods?\n",
      "\n",
      "llm_cot_body:\n",
      "I'm trying to use GitHub Copilot to help me write some code, but it doesn't seem to be working. I'm ...\n",
      "\n",
      "llm_cot_combined:\n",
      "<<How to use GitHub Copilot to generate code for SetupEventHandlers() and InitializePalette() method...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "zero_shot_df = pd.read_csv('Data/Gemini/llm_responses_final_zero_shot.csv')\n",
    "few_shot_df = pd.read_csv('Data/Gemini/llm_responses_final_few_shot.csv')\n",
    "chain_of_thoughts_df = pd.read_csv('Data/Gemini/llm_responses_final_chain_of_thoughts.csv')\n",
    "\n",
    "# Create the combined text column for original title and body\n",
    "zero_shot_df['Title_Body'] = '<<' + zero_shot_df['Title'] + '>>\\n<<' + zero_shot_df['Body'] + '>>'\n",
    "\n",
    "# Create combined response columns for each LLM\n",
    "zero_shot_df['llm_response_combined'] = '<<' + zero_shot_df['llm_title_response'] + '>>\\n<<' + zero_shot_df['llm_body_response'] + '>>'\n",
    "few_shot_df['llm_response_combined'] = '<<' + few_shot_df['llm_title_response'] + '>>\\n<<' + few_shot_df['llm_body_response'] + '>>'\n",
    "chain_of_thoughts_df['llm_response_combined'] = '<<' + chain_of_thoughts_df['llm_title_response'] + '>>\\n<<' + chain_of_thoughts_df['llm_body_response'] + '>>'\n",
    "\n",
    "# Create the final dataframe with all columns\n",
    "combined_df = pd.DataFrame({\n",
    "    'Id': zero_shot_df['Id'],\n",
    "    'Title': zero_shot_df['Title'],\n",
    "    'Body': zero_shot_df['Body'],\n",
    "    'Title_Body': zero_shot_df['Title_Body'],\n",
    "    'ImageURLs': zero_shot_df['ImageURLs'],\n",
    "    # Zero-shot responses\n",
    "    'llm_zero_shot_title': zero_shot_df['llm_title_response'],\n",
    "    'llm_zero_shot_body': zero_shot_df['llm_body_response'],\n",
    "    'llm_zero_shot_combined': zero_shot_df['llm_response_combined'],\n",
    "    # Few-shot responses\n",
    "    'llm_few_shot_title': few_shot_df['llm_title_response'],\n",
    "    'llm_few_shot_body': few_shot_df['llm_body_response'],\n",
    "    'llm_few_shot_combined': few_shot_df['llm_response_combined'],\n",
    "    # Chain of thoughts responses\n",
    "    'llm_cot_title': chain_of_thoughts_df['llm_title_response'],\n",
    "    'llm_cot_body': chain_of_thoughts_df['llm_body_response'],\n",
    "    'llm_cot_combined': chain_of_thoughts_df['llm_response_combined']\n",
    "})\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv('Data/Gemini/llm_responses_combined.csv', index=False)\n",
    "\n",
    "# Print basic information about the combined file\n",
    "print(f\"Combined file created with {len(combined_df)} rows\")\n",
    "print(\"\\nColumns in the combined file:\")\n",
    "for col in combined_df.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "# Print a sample row to verify the structure\n",
    "print(\"\\nSample of first row:\")\n",
    "for col in combined_df.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(str(combined_df[col].iloc[0])[:100] + \"...\" if len(str(combined_df[col].iloc[0])) > 100 else str(combined_df[col].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Title_Body</th>\n",
       "      <th>ImageURLs</th>\n",
       "      <th>llm_zero_shot_title</th>\n",
       "      <th>llm_zero_shot_body</th>\n",
       "      <th>llm_zero_shot_combined</th>\n",
       "      <th>llm_few_shot_title</th>\n",
       "      <th>llm_few_shot_body</th>\n",
       "      <th>llm_few_shot_combined</th>\n",
       "      <th>llm_cot_title</th>\n",
       "      <th>llm_cot_body</th>\n",
       "      <th>llm_cot_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79146548</td>\n",
       "      <td>GitHub Copilot responds to 'Hey Code' but dict...</td>\n",
       "      <td>As the title explains, I can start an inline c...</td>\n",
       "      <td>&lt;&lt;GitHub Copilot responds to 'Hey Code' but di...</td>\n",
       "      <td>['https://i.sstatic.net/MgGjdapB.png']</td>\n",
       "      <td>Why is my IDE/Code editor suggesting \"Ask Copi...</td>\n",
       "      <td>I'm trying to use the \"Ask Copilot\" feature in...</td>\n",
       "      <td>&lt;&lt;Why is my IDE/Code editor suggesting \"Ask Co...</td>\n",
       "      <td>How to Implement Drag and Drop Functionality B...</td>\n",
       "      <td>I am trying to find a reference to the `SetupE...</td>\n",
       "      <td>&lt;&lt;How to Implement Drag and Drop Functionality...</td>\n",
       "      <td>How to use GitHub Copilot to generate code for...</td>\n",
       "      <td>I'm trying to use GitHub Copilot to help me wr...</td>\n",
       "      <td>&lt;&lt;How to use GitHub Copilot to generate code f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79146419</td>\n",
       "      <td>How can I fix my Workflow file to successfully...</td>\n",
       "      <td>I am trying to use Github Actions with Azure S...</td>\n",
       "      <td>&lt;&lt;How can I fix my Workflow file to successful...</td>\n",
       "      <td>['https://i.sstatic.net/THwNK2Jj.png']</td>\n",
       "      <td>\"npm: not found\" error when deploying .NET Cor...</td>\n",
       "      <td>I am trying to deploy my ASP.NET Core applicat...</td>\n",
       "      <td>&lt;&lt;\"npm: not found\" error when deploying .NET C...</td>\n",
       "      <td>Azure DevOps Build Fails: \"The command 'npm ru...</td>\n",
       "      <td>I am trying to publish my .NET Core applicatio...</td>\n",
       "      <td>&lt;&lt;Azure DevOps Build Fails: \"The command 'npm ...</td>\n",
       "      <td>Azure deployment fails with \"npm: not found\" w...</td>\n",
       "      <td>I am trying to deploy my ASP.NET Core applicat...</td>\n",
       "      <td>&lt;&lt;Azure deployment fails with \"npm: not found\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79146412</td>\n",
       "      <td>LINQPad 8 Dump Property Order different that L...</td>\n",
       "      <td>In LINQPad 5 with Linq-to-Sql DataContext, if ...</td>\n",
       "      <td>&lt;&lt;LINQPad 8 Dump Property Order different that...</td>\n",
       "      <td>['https://i.sstatic.net/efq4SfvI.png']</td>\n",
       "      <td>C# - LINQ Select new object with Property Type...</td>\n",
       "      <td>## C# Select Statement Returning Unexpected Pr...</td>\n",
       "      <td>&lt;&lt;C# - LINQ Select new object with Property Ty...</td>\n",
       "      <td>LINQPad Error: No native tree vulnerabilities ...</td>\n",
       "      <td>I am receiving a LINQPad error message stating...</td>\n",
       "      <td>&lt;&lt;LINQPad Error: No native tree vulnerabilitie...</td>\n",
       "      <td>How to Select Specific Properties from a User ...</td>\n",
       "      <td>I'm trying to retrieve user properties and the...</td>\n",
       "      <td>&lt;&lt;How to Select Specific Properties from a Use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79146127</td>\n",
       "      <td>SyntaxError: Cannot use import statement outsi...</td>\n",
       "      <td>I'm using TypeScript, ESM, npm, and ts-jest. U...</td>\n",
       "      <td>&lt;&lt;SyntaxError: Cannot use import statement out...</td>\n",
       "      <td>['https://i.sstatic.net/Jp5wj6k2.png']</td>\n",
       "      <td>\"SyntaxError: Cannot use import statement outs...</td>\n",
       "      <td>## SyntaxError: Cannot use import statement ou...</td>\n",
       "      <td>&lt;&lt;\"SyntaxError: Cannot use import statement ou...</td>\n",
       "      <td>How to Fix \"SyntaxError: Cannot use import sta...</td>\n",
       "      <td>I am trying to import `chalk` and `picocolors`...</td>\n",
       "      <td>&lt;&lt;How to Fix \"SyntaxError: Cannot use import s...</td>\n",
       "      <td>\"SyntaxError: Cannot use import statement outs...</td>\n",
       "      <td>I'm encountering a \"SyntaxError: Cannot use im...</td>\n",
       "      <td>&lt;&lt;\"SyntaxError: Cannot use import statement ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79145758</td>\n",
       "      <td>Typescript Polymorphic Component Event Handler</td>\n",
       "      <td>I have written a strongly-typed Polymorphic Ty...</td>\n",
       "      <td>&lt;&lt;Typescript Polymorphic Component Event Handl...</td>\n",
       "      <td>['https://i.sstatic.net/19LCKEF3.png']</td>\n",
       "      <td>TypeScript Error: Property 'currentTarget' doe...</td>\n",
       "      <td>## TypeScript error: \"Property 'currentTarget'...</td>\n",
       "      <td>&lt;&lt;TypeScript Error: Property 'currentTarget' d...</td>\n",
       "      <td>Property 'currentTarget' does not exist on typ...</td>\n",
       "      <td>I am working with a React project using TypeSc...</td>\n",
       "      <td>&lt;&lt;Property 'currentTarget' does not exist on t...</td>\n",
       "      <td>Property 'currentTarget' does not exist on typ...</td>\n",
       "      <td>I'm encountering a TypeScript error in my Reac...</td>\n",
       "      <td>&lt;&lt;Property 'currentTarget' does not exist on t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  79146548  GitHub Copilot responds to 'Hey Code' but dict...   \n",
       "1  79146419  How can I fix my Workflow file to successfully...   \n",
       "2  79146412  LINQPad 8 Dump Property Order different that L...   \n",
       "3  79146127  SyntaxError: Cannot use import statement outsi...   \n",
       "4  79145758     Typescript Polymorphic Component Event Handler   \n",
       "\n",
       "                                                Body  \\\n",
       "0  As the title explains, I can start an inline c...   \n",
       "1  I am trying to use Github Actions with Azure S...   \n",
       "2  In LINQPad 5 with Linq-to-Sql DataContext, if ...   \n",
       "3  I'm using TypeScript, ESM, npm, and ts-jest. U...   \n",
       "4  I have written a strongly-typed Polymorphic Ty...   \n",
       "\n",
       "                                          Title_Body  \\\n",
       "0  <<GitHub Copilot responds to 'Hey Code' but di...   \n",
       "1  <<How can I fix my Workflow file to successful...   \n",
       "2  <<LINQPad 8 Dump Property Order different that...   \n",
       "3  <<SyntaxError: Cannot use import statement out...   \n",
       "4  <<Typescript Polymorphic Component Event Handl...   \n",
       "\n",
       "                                ImageURLs  \\\n",
       "0  ['https://i.sstatic.net/MgGjdapB.png']   \n",
       "1  ['https://i.sstatic.net/THwNK2Jj.png']   \n",
       "2  ['https://i.sstatic.net/efq4SfvI.png']   \n",
       "3  ['https://i.sstatic.net/Jp5wj6k2.png']   \n",
       "4  ['https://i.sstatic.net/19LCKEF3.png']   \n",
       "\n",
       "                                 llm_zero_shot_title  \\\n",
       "0  Why is my IDE/Code editor suggesting \"Ask Copi...   \n",
       "1  \"npm: not found\" error when deploying .NET Cor...   \n",
       "2  C# - LINQ Select new object with Property Type...   \n",
       "3  \"SyntaxError: Cannot use import statement outs...   \n",
       "4  TypeScript Error: Property 'currentTarget' doe...   \n",
       "\n",
       "                                  llm_zero_shot_body  \\\n",
       "0  I'm trying to use the \"Ask Copilot\" feature in...   \n",
       "1  I am trying to deploy my ASP.NET Core applicat...   \n",
       "2  ## C# Select Statement Returning Unexpected Pr...   \n",
       "3  ## SyntaxError: Cannot use import statement ou...   \n",
       "4  ## TypeScript error: \"Property 'currentTarget'...   \n",
       "\n",
       "                              llm_zero_shot_combined  \\\n",
       "0  <<Why is my IDE/Code editor suggesting \"Ask Co...   \n",
       "1  <<\"npm: not found\" error when deploying .NET C...   \n",
       "2  <<C# - LINQ Select new object with Property Ty...   \n",
       "3  <<\"SyntaxError: Cannot use import statement ou...   \n",
       "4  <<TypeScript Error: Property 'currentTarget' d...   \n",
       "\n",
       "                                  llm_few_shot_title  \\\n",
       "0  How to Implement Drag and Drop Functionality B...   \n",
       "1  Azure DevOps Build Fails: \"The command 'npm ru...   \n",
       "2  LINQPad Error: No native tree vulnerabilities ...   \n",
       "3  How to Fix \"SyntaxError: Cannot use import sta...   \n",
       "4  Property 'currentTarget' does not exist on typ...   \n",
       "\n",
       "                                   llm_few_shot_body  \\\n",
       "0  I am trying to find a reference to the `SetupE...   \n",
       "1  I am trying to publish my .NET Core applicatio...   \n",
       "2  I am receiving a LINQPad error message stating...   \n",
       "3  I am trying to import `chalk` and `picocolors`...   \n",
       "4  I am working with a React project using TypeSc...   \n",
       "\n",
       "                               llm_few_shot_combined  \\\n",
       "0  <<How to Implement Drag and Drop Functionality...   \n",
       "1  <<Azure DevOps Build Fails: \"The command 'npm ...   \n",
       "2  <<LINQPad Error: No native tree vulnerabilitie...   \n",
       "3  <<How to Fix \"SyntaxError: Cannot use import s...   \n",
       "4  <<Property 'currentTarget' does not exist on t...   \n",
       "\n",
       "                                       llm_cot_title  \\\n",
       "0  How to use GitHub Copilot to generate code for...   \n",
       "1  Azure deployment fails with \"npm: not found\" w...   \n",
       "2  How to Select Specific Properties from a User ...   \n",
       "3  \"SyntaxError: Cannot use import statement outs...   \n",
       "4  Property 'currentTarget' does not exist on typ...   \n",
       "\n",
       "                                        llm_cot_body  \\\n",
       "0  I'm trying to use GitHub Copilot to help me wr...   \n",
       "1  I am trying to deploy my ASP.NET Core applicat...   \n",
       "2  I'm trying to retrieve user properties and the...   \n",
       "3  I'm encountering a \"SyntaxError: Cannot use im...   \n",
       "4  I'm encountering a TypeScript error in my Reac...   \n",
       "\n",
       "                                    llm_cot_combined  \n",
       "0  <<How to use GitHub Copilot to generate code f...  \n",
       "1  <<Azure deployment fails with \"npm: not found\"...  \n",
       "2  <<How to Select Specific Properties from a Use...  \n",
       "3  <<\"SyntaxError: Cannot use import statement ou...  \n",
       "4  <<Property 'currentTarget' does not exist on t...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for original content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f118967da5434885a2d71dfe061fd4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84decbce1cd4947b9a97f15a3ce38ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9342b5a7669f4fe6a9330b16285abb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for LLM responses...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc332b3149ea42c5a95b4a2dca25504e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54554c24ee74b8084a9455a0bd5c09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1574eedc883e42918b061b4aefe607d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99819774256c4266868548c1efb2e20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3fe6ce839148c9b6c7be8ed551a3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9d34b7e0884be49a8ddfdf08a4204d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818c1da3c7f24d589d5083dfa4337a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce67402f3ee648e5a4b69a0434accc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353733d90a2a496783475a1a85c0f832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TITLE Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4581\n",
      "median: 0.4816\n",
      "std: 0.2075\n",
      "min: -0.0375\n",
      "max: 0.8861\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 3.50%\n",
      "High: 25.87%\n",
      "Moderate: 32.17%\n",
      "Low: 24.48%\n",
      "Very Low: 12.59%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.3552\n",
      "median: 0.3461\n",
      "std: 0.2145\n",
      "min: -0.1036\n",
      "max: 0.9188\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 1.40%\n",
      "High: 14.69%\n",
      "Moderate: 27.27%\n",
      "Low: 27.97%\n",
      "Very Low: 26.57%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4512\n",
      "median: 0.4544\n",
      "std: 0.2340\n",
      "min: -0.0135\n",
      "max: 0.9497\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 6.99%\n",
      "High: 25.17%\n",
      "Moderate: 24.48%\n",
      "Low: 25.17%\n",
      "Very Low: 16.78%\n",
      "\n",
      "BODY Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4923\n",
      "median: 0.5016\n",
      "std: 0.2138\n",
      "min: -0.0008\n",
      "max: 0.8823\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 5.59%\n",
      "High: 30.07%\n",
      "Moderate: 32.87%\n",
      "Low: 18.88%\n",
      "Very Low: 11.89%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4208\n",
      "median: 0.4270\n",
      "std: 0.2038\n",
      "min: -0.0833\n",
      "max: 0.8904\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 2.10%\n",
      "High: 20.98%\n",
      "Moderate: 31.47%\n",
      "Low: 30.07%\n",
      "Very Low: 13.99%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4977\n",
      "median: 0.5038\n",
      "std: 0.2121\n",
      "min: -0.0810\n",
      "max: 0.8905\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 6.29%\n",
      "High: 30.07%\n",
      "Moderate: 32.87%\n",
      "Low: 22.38%\n",
      "Very Low: 6.99%\n",
      "\n",
      "COMBINED Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5449\n",
      "median: 0.5692\n",
      "std: 0.2080\n",
      "min: 0.0076\n",
      "max: 0.8905\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 9.09%\n",
      "High: 34.27%\n",
      "Moderate: 32.17%\n",
      "Low: 16.78%\n",
      "Very Low: 7.69%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4923\n",
      "median: 0.5205\n",
      "std: 0.2032\n",
      "min: 0.0325\n",
      "max: 0.8773\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 4.90%\n",
      "High: 30.77%\n",
      "Moderate: 30.77%\n",
      "Low: 25.17%\n",
      "Very Low: 8.39%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5703\n",
      "median: 0.6231\n",
      "std: 0.2044\n",
      "min: 0.0076\n",
      "max: 0.9056\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 13.29%\n",
      "High: 37.06%\n",
      "Moderate: 26.57%\n",
      "Low: 17.48%\n",
      "Very Low: 5.59%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_embeddings_and_analyze(df, model_name='sentence-transformers/all-mpnet-base-v1'):\n",
    "    \"\"\"\n",
    "    Create embeddings using SentenceTransformer and analyze similarities for both\n",
    "    separate title/body responses and combined responses\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings for original content\n",
    "    print(\"Generating embeddings for original content...\")\n",
    "    title_embeddings = model.encode(df['Title'].tolist(), show_progress_bar=True)\n",
    "    body_embeddings = model.encode(df['Body'].tolist(), show_progress_bar=True)\n",
    "    title_body_embeddings = model.encode(df['Title_Body'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    # Generate embeddings for LLM responses\n",
    "    print(\"\\nGenerating embeddings for LLM responses...\")\n",
    "    response_embeddings = {\n",
    "        # Title responses\n",
    "        'zero_shot_title': model.encode(df['llm_zero_shot_title'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_title': model.encode(df['llm_few_shot_title'].tolist(), show_progress_bar=True),\n",
    "        'cot_title': model.encode(df['llm_cot_title'].tolist(), show_progress_bar=True),\n",
    "        \n",
    "        # Body responses\n",
    "        'zero_shot_body': model.encode(df['llm_zero_shot_body'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_body': model.encode(df['llm_few_shot_body'].tolist(), show_progress_bar=True),\n",
    "        'cot_body': model.encode(df['llm_cot_body'].tolist(), show_progress_bar=True),\n",
    "        \n",
    "        # Combined responses\n",
    "        'zero_shot_combined': model.encode(df['llm_zero_shot_combined'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_combined': model.encode(df['llm_few_shot_combined'].tolist(), show_progress_bar=True),\n",
    "        'cot_combined': model.encode(df['llm_cot_combined'].tolist(), show_progress_bar=True)\n",
    "    }\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate title similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['title'][response_type] = np.diagonal(\n",
    "            cosine_similarity(title_embeddings, response_embeddings[f'{response_type}_title'])\n",
    "        )\n",
    "    \n",
    "    # Calculate body similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['body'][response_type] = np.diagonal(\n",
    "            cosine_similarity(body_embeddings, response_embeddings[f'{response_type}_body'])\n",
    "        )\n",
    "    \n",
    "    # Calculate combined similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['combined'][response_type] = np.diagonal(\n",
    "            cosine_similarity(title_body_embeddings, response_embeddings[f'{response_type}_combined'])\n",
    "        )\n",
    "    \n",
    "    # Add similarity scores to dataframe\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        df[f'similarity_{response_type}_title'] = similarities['title'][response_type]\n",
    "        df[f'similarity_{response_type}_body'] = similarities['body'][response_type]\n",
    "        df[f'similarity_{response_type}_combined'] = similarities['combined'][response_type]\n",
    "    \n",
    "    # Save embeddings\n",
    "    np.save('Data/title_embeddings_st.npy', title_embeddings)\n",
    "    np.save('Data/body_embeddings_st.npy', body_embeddings)\n",
    "    np.save('Data/title_body_embeddings_st.npy', title_body_embeddings)\n",
    "    \n",
    "    for response_type, embeddings in response_embeddings.items():\n",
    "        np.save(f'Data/{response_type}_embeddings_st.npy', embeddings)\n",
    "    \n",
    "    return df, similarities\n",
    "\n",
    "def analyze_similarities(similarities):\n",
    "    \"\"\"\n",
    "    Analyze and visualize similarity distributions for title, body, and combined responses\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        'Very High': (0.8, 1.0),\n",
    "        'High': (0.6, 0.8),\n",
    "        'Moderate': (0.4, 0.6),\n",
    "        'Low': (0.2, 0.4),\n",
    "        'Very Low': (0.0, 0.2)\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Create figures for each type of response\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle(f'Similarity Analysis for {response_category.capitalize()} Responses')\n",
    "        \n",
    "        # Plot distributions and calculate statistics for each LLM type\n",
    "        for idx, response_type in enumerate(['zero_shot', 'few_shot', 'cot']):\n",
    "            scores = similarities[response_category][response_type]\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = {\n",
    "                'mean': np.mean(scores),\n",
    "                'median': np.median(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "            \n",
    "            # Calculate distribution across categories\n",
    "            distribution = {}\n",
    "            for category, (low, high) in categories.items():\n",
    "                count = np.sum((scores >= low) & (scores < high))\n",
    "                percentage = (count / len(scores)) * 100\n",
    "                distribution[category] = percentage\n",
    "                \n",
    "            results[response_category][response_type] = {\n",
    "                'statistics': stats,\n",
    "                'distribution': distribution\n",
    "            }\n",
    "            \n",
    "            # Plot histogram\n",
    "            ax = axes[idx]\n",
    "            sns.histplot(scores, bins=30, ax=ax)\n",
    "            ax.set_title(f'{response_type.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel('Similarity Score')\n",
    "            ax.set_ylabel('Count')\n",
    "            \n",
    "            # Add category boundaries\n",
    "            for category, (low, high) in categories.items():\n",
    "                if low > 0:  # Don't plot the lowest boundary\n",
    "                    ax.axvline(x=low, color='r', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'Data/similarity_distributions_{response_category}_st.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Print analysis\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        print(f\"\\n{response_category.upper()} Response Analysis:\")\n",
    "        for response_type, result in results[response_category].items():\n",
    "            print(f\"\\n{response_type.upper()}:\")\n",
    "            print(\"\\nBasic Statistics:\")\n",
    "            for metric, value in result['statistics'].items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            \n",
    "            print(\"\\nDistribution across categories:\")\n",
    "            for category, percentage in result['distribution'].items():\n",
    "                print(f\"{category}: {percentage:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the combined CSV file\n",
    "    df = pd.read_csv('Data/Gemini/llm_responses_combined.csv')\n",
    "    \n",
    "    # Generate embeddings and calculate similarities\n",
    "    df_with_similarities, similarities = create_embeddings_and_analyze(df)\n",
    "    \n",
    "    # Analyze and visualize results\n",
    "    analysis_results = analyze_similarities(similarities)\n",
    "    \n",
    "    # Save updated dataframe with similarities\n",
    "    df_with_similarities.to_csv('Data/Gemini/llm_responses_with_similarities_st.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for original content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8451bc8573fe4783a45c05e80de3a4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e527997a6ac24e738a03ff67dade9bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7363be993e884e2497e83976587c9056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for LLM responses...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b410b9aee34cd39caa3d82bc9db2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96cb4d830e54c7ab733f92120f2d2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63aaa75643fa4604918e5eaafc3d297d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13771ceec5b4e6295615a3800b0cd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5724c3dbd3954756b24fdb2b4cbf0eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b65e25927a4442b85b136b5b7ef955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c8ff2c9965427da5465cd7d5bb2588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56217e244234432ac49af92e9aec46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4b68100c464935b0b536a49be12293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TITLE Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4720\n",
      "median: 0.4942\n",
      "std: 0.2152\n",
      "min: -0.0771\n",
      "max: 0.9190\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 2.80%\n",
      "High: 30.77%\n",
      "Moderate: 30.07%\n",
      "Low: 24.48%\n",
      "Very Low: 11.19%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.3751\n",
      "median: 0.3910\n",
      "std: 0.2242\n",
      "min: -0.1247\n",
      "max: 0.9602\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 2.80%\n",
      "High: 14.69%\n",
      "Moderate: 27.97%\n",
      "Low: 29.37%\n",
      "Very Low: 21.68%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4654\n",
      "median: 0.4642\n",
      "std: 0.2299\n",
      "min: 0.0050\n",
      "max: 0.9656\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 8.39%\n",
      "High: 25.87%\n",
      "Moderate: 25.17%\n",
      "Low: 25.87%\n",
      "Very Low: 14.69%\n",
      "\n",
      "BODY Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5186\n",
      "median: 0.5462\n",
      "std: 0.2058\n",
      "min: -0.0848\n",
      "max: 0.9123\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 8.39%\n",
      "High: 27.97%\n",
      "Moderate: 35.66%\n",
      "Low: 21.68%\n",
      "Very Low: 4.20%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.4432\n",
      "median: 0.4576\n",
      "std: 0.1916\n",
      "min: 0.0127\n",
      "max: 0.8875\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 3.50%\n",
      "High: 16.78%\n",
      "Moderate: 39.86%\n",
      "Low: 27.27%\n",
      "Very Low: 12.59%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5240\n",
      "median: 0.5526\n",
      "std: 0.2014\n",
      "min: -0.0848\n",
      "max: 0.8909\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 7.69%\n",
      "High: 32.87%\n",
      "Moderate: 31.47%\n",
      "Low: 22.38%\n",
      "Very Low: 4.90%\n",
      "\n",
      "COMBINED Response Analysis:\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5757\n",
      "median: 0.5959\n",
      "std: 0.1996\n",
      "min: -0.0773\n",
      "max: 0.9134\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 11.19%\n",
      "High: 38.46%\n",
      "Moderate: 34.97%\n",
      "Low: 11.89%\n",
      "Very Low: 2.10%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5176\n",
      "median: 0.5438\n",
      "std: 0.1907\n",
      "min: 0.0821\n",
      "max: 0.9111\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 6.29%\n",
      "High: 30.07%\n",
      "Moderate: 38.46%\n",
      "Low: 20.98%\n",
      "Very Low: 4.20%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5962\n",
      "median: 0.6170\n",
      "std: 0.1957\n",
      "min: -0.0773\n",
      "max: 0.9129\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 13.99%\n",
      "High: 42.66%\n",
      "Moderate: 29.37%\n",
      "Low: 9.09%\n",
      "Very Low: 4.20%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_embeddings_and_analyze(df, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Create embeddings using SentenceTransformer and analyze similarities for both\n",
    "    separate title/body responses and combined responses\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings for original content\n",
    "    print(\"Generating embeddings for original content...\")\n",
    "    title_embeddings = model.encode(df['Title'].tolist(), show_progress_bar=True)\n",
    "    body_embeddings = model.encode(df['Body'].tolist(), show_progress_bar=True)\n",
    "    title_body_embeddings = model.encode(df['Title_Body'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    # Generate embeddings for LLM responses\n",
    "    print(\"\\nGenerating embeddings for LLM responses...\")\n",
    "    response_embeddings = {\n",
    "        # Title responses\n",
    "        'zero_shot_title': model.encode(df['llm_zero_shot_title'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_title': model.encode(df['llm_few_shot_title'].tolist(), show_progress_bar=True),\n",
    "        'cot_title': model.encode(df['llm_cot_title'].tolist(), show_progress_bar=True),\n",
    "        \n",
    "        # Body responses\n",
    "        'zero_shot_body': model.encode(df['llm_zero_shot_body'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_body': model.encode(df['llm_few_shot_body'].tolist(), show_progress_bar=True),\n",
    "        'cot_body': model.encode(df['llm_cot_body'].tolist(), show_progress_bar=True),\n",
    "        \n",
    "        # Combined responses\n",
    "        'zero_shot_combined': model.encode(df['llm_zero_shot_combined'].tolist(), show_progress_bar=True),\n",
    "        'few_shot_combined': model.encode(df['llm_few_shot_combined'].tolist(), show_progress_bar=True),\n",
    "        'cot_combined': model.encode(df['llm_cot_combined'].tolist(), show_progress_bar=True)\n",
    "    }\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate title similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['title'][response_type] = np.diagonal(\n",
    "            cosine_similarity(title_embeddings, response_embeddings[f'{response_type}_title'])\n",
    "        )\n",
    "    \n",
    "    # Calculate body similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['body'][response_type] = np.diagonal(\n",
    "            cosine_similarity(body_embeddings, response_embeddings[f'{response_type}_body'])\n",
    "        )\n",
    "    \n",
    "    # Calculate combined similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['combined'][response_type] = np.diagonal(\n",
    "            cosine_similarity(title_body_embeddings, response_embeddings[f'{response_type}_combined'])\n",
    "        )\n",
    "    \n",
    "    # Add similarity scores to dataframe\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        df[f'similarity_{response_type}_title'] = similarities['title'][response_type]\n",
    "        df[f'similarity_{response_type}_body'] = similarities['body'][response_type]\n",
    "        df[f'similarity_{response_type}_combined'] = similarities['combined'][response_type]\n",
    "    \n",
    "    # Save embeddings\n",
    "    np.save('Data/title_embeddings_st.npy', title_embeddings)\n",
    "    np.save('Data/body_embeddings_st.npy', body_embeddings)\n",
    "    np.save('Data/title_body_embeddings_st.npy', title_body_embeddings)\n",
    "    \n",
    "    for response_type, embeddings in response_embeddings.items():\n",
    "        np.save(f'Data/{response_type}_embeddings_st.npy', embeddings)\n",
    "    \n",
    "    return df, similarities\n",
    "\n",
    "def analyze_similarities(similarities):\n",
    "    \"\"\"\n",
    "    Analyze and visualize similarity distributions for title, body, and combined responses\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        'Very High': (0.8, 1.0),\n",
    "        'High': (0.6, 0.8),\n",
    "        'Moderate': (0.4, 0.6),\n",
    "        'Low': (0.2, 0.4),\n",
    "        'Very Low': (0.0, 0.2)\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Create figures for each type of response\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle(f'Similarity Analysis for {response_category.capitalize()} Responses')\n",
    "        \n",
    "        # Plot distributions and calculate statistics for each LLM type\n",
    "        for idx, response_type in enumerate(['zero_shot', 'few_shot', 'cot']):\n",
    "            scores = similarities[response_category][response_type]\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = {\n",
    "                'mean': np.mean(scores),\n",
    "                'median': np.median(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "            \n",
    "            # Calculate distribution across categories\n",
    "            distribution = {}\n",
    "            for category, (low, high) in categories.items():\n",
    "                count = np.sum((scores >= low) & (scores < high))\n",
    "                percentage = (count / len(scores)) * 100\n",
    "                distribution[category] = percentage\n",
    "                \n",
    "            results[response_category][response_type] = {\n",
    "                'statistics': stats,\n",
    "                'distribution': distribution\n",
    "            }\n",
    "            \n",
    "            # Plot histogram\n",
    "            ax = axes[idx]\n",
    "            sns.histplot(scores, bins=30, ax=ax)\n",
    "            ax.set_title(f'{response_type.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel('Similarity Score')\n",
    "            ax.set_ylabel('Count')\n",
    "            \n",
    "            # Add category boundaries\n",
    "            for category, (low, high) in categories.items():\n",
    "                if low > 0:  # Don't plot the lowest boundary\n",
    "                    ax.axvline(x=low, color='r', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'Data/similarity_distributions_{response_category}_st.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Print analysis\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        print(f\"\\n{response_category.upper()} Response Analysis:\")\n",
    "        for response_type, result in results[response_category].items():\n",
    "            print(f\"\\n{response_type.upper()}:\")\n",
    "            print(\"\\nBasic Statistics:\")\n",
    "            for metric, value in result['statistics'].items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            \n",
    "            print(\"\\nDistribution across categories:\")\n",
    "            for category, percentage in result['distribution'].items():\n",
    "                print(f\"{category}: {percentage:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the combined CSV file\n",
    "    df = pd.read_csv('Data/Gemini/llm_responses_combined.csv')\n",
    "    \n",
    "    # Generate embeddings and calculate similarities\n",
    "    df_with_similarities, similarities = create_embeddings_and_analyze(df)\n",
    "    \n",
    "    # Analyze and visualize results\n",
    "    analysis_results = analyze_similarities(similarities)\n",
    "    \n",
    "    # Save updated dataframe with similarities\n",
    "    df_with_similarities.to_csv('Data/Gemini/llm_responses_with_similarities_st.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13081a9eed0440ea96f7e28aaa06aba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5e4e8af9d344a4bdd8decd096e71a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/499k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bce350f9f984930ad0dd1a83d4d86b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1764c20a7e4add9eeaa4e66da71374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/741 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10698a056c4a4a79b5a6ab4ffaccf806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/415M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b090415def054060b1c0ba7a523cb84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d98d3dac8d460292bb50b517352008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/176k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a53fbe910246e9bf7d47eb10922bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/535k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61686b8432944da983e25ebac272fddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e3395015254d4e9479549a92464ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for original content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4bf8232783433695debf33bb9a17be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aada1627dca148ec973e6189c04aec81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c147630824ad4890b85e50439bfbb209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for LLM responses...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab97fa6eecf4b9997da779f4ad57a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81071fb05f77467b8fc3c910275c9cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8fb61bf4d34119b7fb01670bca5c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ec050fa5e34614952ca7ff77994180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72af27ba1b0c488e82d4ce67d7da427d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d468d4ed0e49dd856990807861377c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9480504e69a7461992696941d4d96c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020a2733310b4f5891d0d3fbd9637536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f0e24ee8424ddba80c4ebeeaf80331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TITLE Response Analysis (KartonBERT):\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.6121\n",
      "median: 0.6075\n",
      "std: 0.1376\n",
      "min: 0.2549\n",
      "max: 0.9097\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 11.19%\n",
      "High: 41.26%\n",
      "Moderate: 42.66%\n",
      "Low: 4.90%\n",
      "Very Low: 0.00%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.5506\n",
      "median: 0.5403\n",
      "std: 0.1565\n",
      "min: 0.1331\n",
      "max: 0.9077\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 8.39%\n",
      "High: 28.67%\n",
      "Moderate: 46.15%\n",
      "Low: 15.38%\n",
      "Very Low: 1.40%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.6026\n",
      "median: 0.6009\n",
      "std: 0.1582\n",
      "min: 0.2320\n",
      "max: 0.9381\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 13.29%\n",
      "High: 37.06%\n",
      "Moderate: 38.46%\n",
      "Low: 11.19%\n",
      "Very Low: 0.00%\n",
      "\n",
      "BODY Response Analysis (KartonBERT):\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.6950\n",
      "median: 0.7261\n",
      "std: 0.1323\n",
      "min: 0.2278\n",
      "max: 0.9387\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 17.48%\n",
      "High: 61.54%\n",
      "Moderate: 17.48%\n",
      "Low: 3.50%\n",
      "Very Low: 0.00%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.6448\n",
      "median: 0.6551\n",
      "std: 0.1139\n",
      "min: 0.3176\n",
      "max: 0.9371\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 9.79%\n",
      "High: 56.64%\n",
      "Moderate: 32.17%\n",
      "Low: 1.40%\n",
      "Very Low: 0.00%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.7105\n",
      "median: 0.7315\n",
      "std: 0.1207\n",
      "min: 0.3468\n",
      "max: 0.9332\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 24.48%\n",
      "High: 57.34%\n",
      "Moderate: 16.08%\n",
      "Low: 2.10%\n",
      "Very Low: 0.00%\n",
      "\n",
      "COMBINED Response Analysis (KartonBERT):\n",
      "\n",
      "ZERO_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.7201\n",
      "median: 0.7397\n",
      "std: 0.1336\n",
      "min: 0.2073\n",
      "max: 0.9427\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 29.37%\n",
      "High: 56.64%\n",
      "Moderate: 10.49%\n",
      "Low: 3.50%\n",
      "Very Low: 0.00%\n",
      "\n",
      "FEW_SHOT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.6689\n",
      "median: 0.6853\n",
      "std: 0.1182\n",
      "min: 0.3776\n",
      "max: 0.9492\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 13.29%\n",
      "High: 59.44%\n",
      "Moderate: 25.17%\n",
      "Low: 2.10%\n",
      "Very Low: 0.00%\n",
      "\n",
      "COT:\n",
      "\n",
      "Basic Statistics:\n",
      "mean: 0.7307\n",
      "median: 0.7424\n",
      "std: 0.1233\n",
      "min: 0.3737\n",
      "max: 0.9468\n",
      "\n",
      "Distribution across categories:\n",
      "Very High: 32.87%\n",
      "High: 53.15%\n",
      "Moderate: 11.89%\n",
      "Low: 2.10%\n",
      "Very Low: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_embeddings_and_analyze(df, model_name='OrlikB/KartonBERT-USE-base-v1'):\n",
    "    \"\"\"\n",
    "    Create embeddings using KartonBERT and analyze similarities with normalized embeddings\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings for original content\n",
    "    print(\"Generating embeddings for original content...\")\n",
    "    title_embeddings = model.encode(df['Title'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    body_embeddings = model.encode(df['Body'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    title_body_embeddings = model.encode(df['Title_Body'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    \n",
    "    # Generate embeddings for LLM responses\n",
    "    print(\"\\nGenerating embeddings for LLM responses...\")\n",
    "    response_embeddings = {\n",
    "        # Title responses\n",
    "        'zero_shot_title': model.encode(df['llm_zero_shot_title'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'few_shot_title': model.encode(df['llm_few_shot_title'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'cot_title': model.encode(df['llm_cot_title'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        \n",
    "        # Body responses\n",
    "        'zero_shot_body': model.encode(df['llm_zero_shot_body'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'few_shot_body': model.encode(df['llm_few_shot_body'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'cot_body': model.encode(df['llm_cot_body'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        \n",
    "        # Combined responses\n",
    "        'zero_shot_combined': model.encode(df['llm_zero_shot_combined'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'few_shot_combined': model.encode(df['llm_few_shot_combined'].tolist(), normalize_embeddings=True, show_progress_bar=True),\n",
    "        'cot_combined': model.encode(df['llm_cot_combined'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "    }\n",
    "    \n",
    "    # Calculate similarities using dot product\n",
    "    similarities = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate title similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        # Using dot product for normalized vectors\n",
    "        similarities['title'][response_type] = np.sum(\n",
    "            title_embeddings * response_embeddings[f'{response_type}_title'], axis=1\n",
    "        )\n",
    "    \n",
    "    # Calculate body similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['body'][response_type] = np.sum(\n",
    "            body_embeddings * response_embeddings[f'{response_type}_body'], axis=1\n",
    "        )\n",
    "    \n",
    "    # Calculate combined similarities\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        similarities['combined'][response_type] = np.sum(\n",
    "            title_body_embeddings * response_embeddings[f'{response_type}_combined'], axis=1\n",
    "        )\n",
    "    \n",
    "    # Add similarity scores to dataframe\n",
    "    for response_type in ['zero_shot', 'few_shot', 'cot']:\n",
    "        df[f'similarity_{response_type}_title'] = similarities['title'][response_type]\n",
    "        df[f'similarity_{response_type}_body'] = similarities['body'][response_type]\n",
    "        df[f'similarity_{response_type}_combined'] = similarities['combined'][response_type]\n",
    "    \n",
    "    # Save embeddings\n",
    "    np.save('Data/title_embeddings_kartonbert.npy', title_embeddings)\n",
    "    np.save('Data/body_embeddings_kartonbert.npy', body_embeddings)\n",
    "    np.save('Data/title_body_embeddings_kartonbert.npy', title_body_embeddings)\n",
    "    \n",
    "    for response_type, embeddings in response_embeddings.items():\n",
    "        np.save(f'Data/{response_type}_embeddings_kartonbert.npy', embeddings)\n",
    "    \n",
    "    return df, similarities\n",
    "\n",
    "def analyze_similarities(similarities):\n",
    "    \"\"\"\n",
    "    Analyze and visualize similarity distributions for title, body, and combined responses\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        'Very High': (0.8, 1.0),\n",
    "        'High': (0.6, 0.8),\n",
    "        'Moderate': (0.4, 0.6),\n",
    "        'Low': (0.2, 0.4),\n",
    "        'Very Low': (0.0, 0.2)\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        'title': {},\n",
    "        'body': {},\n",
    "        'combined': {}\n",
    "    }\n",
    "    \n",
    "    # Create figures for each type of response\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle(f'Similarity Analysis for {response_category.capitalize()} Responses (KartonBERT)')\n",
    "        \n",
    "        # Plot distributions and calculate statistics for each LLM type\n",
    "        for idx, response_type in enumerate(['zero_shot', 'few_shot', 'cot']):\n",
    "            scores = similarities[response_category][response_type]\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = {\n",
    "                'mean': np.mean(scores),\n",
    "                'median': np.median(scores),\n",
    "                'std': np.std(scores),\n",
    "                'min': np.min(scores),\n",
    "                'max': np.max(scores)\n",
    "            }\n",
    "            \n",
    "            # Calculate distribution across categories\n",
    "            distribution = {}\n",
    "            for category, (low, high) in categories.items():\n",
    "                count = np.sum((scores >= low) & (scores < high))\n",
    "                percentage = (count / len(scores)) * 100\n",
    "                distribution[category] = percentage\n",
    "                \n",
    "            results[response_category][response_type] = {\n",
    "                'statistics': stats,\n",
    "                'distribution': distribution\n",
    "            }\n",
    "            \n",
    "            # Plot histogram\n",
    "            ax = axes[idx]\n",
    "            sns.histplot(scores, bins=30, ax=ax)\n",
    "            ax.set_title(f'{response_type.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel('Similarity Score')\n",
    "            ax.set_ylabel('Count')\n",
    "            \n",
    "            # Add category boundaries\n",
    "            for category, (low, high) in categories.items():\n",
    "                if low > 0:  # Don't plot the lowest boundary\n",
    "                    ax.axvline(x=low, color='r', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'Data/similarity_distributions_{response_category}_kartonbert.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Print analysis\n",
    "    for response_category in ['title', 'body', 'combined']:\n",
    "        print(f\"\\n{response_category.upper()} Response Analysis (KartonBERT):\")\n",
    "        for response_type, result in results[response_category].items():\n",
    "            print(f\"\\n{response_type.upper()}:\")\n",
    "            print(\"\\nBasic Statistics:\")\n",
    "            for metric, value in result['statistics'].items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            \n",
    "            print(\"\\nDistribution across categories:\")\n",
    "            for category, percentage in result['distribution'].items():\n",
    "                print(f\"{category}: {percentage:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the combined CSV file\n",
    "    df = pd.read_csv('Data/Gemini/llm_responses_combined.csv')\n",
    "    \n",
    "    # Generate embeddings and calculate similarities\n",
    "    df_with_similarities, similarities = create_embeddings_and_analyze(df)\n",
    "    \n",
    "    # Analyze and visualize results\n",
    "    analysis_results = analyze_similarities(similarities)\n",
    "    \n",
    "    # Save updated dataframe with similarities\n",
    "    df_with_similarities.to_csv('Data/Gemini/llm_responses_with_similarities_kartonbert.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
